使用node10 在4张显卡上跑代码
node10               Thu Aug 24 20:53:35 2023  515.65.01
[0] NVIDIA A40       | 59'C,   0 % |   573 / 46068 MB |
[1] NVIDIA A40       | 63'C,   0 % |   573 / 46068 MB |
[2] NVIDIA A40       | 57'C,   0 % |   573 / 46068 MB |
[3] NVIDIA A40       | 61'C,   0 % |   573 / 46068 MB |
+ set -e
+ TASK=WN18RR
+++ dirname scripts/train_wn.sh
++ cd scripts
++ cd ..
++ pwd
+ DIR=/home/jiangyunqi/KGC/SimKGC
+ echo 'working directory: /home/jiangyunqi/KGC/SimKGC'
working directory: /home/jiangyunqi/KGC/SimKGC
+ '[' -z ./checkpoint/wn18rr/ ']'
+ '[' -z '' ']'
+ DATA_DIR=/home/jiangyunqi/KGC/SimKGC/data/WN18RR
+ python3 -u main.py --model-dir ./checkpoint/wn18rr/ --pretrained-model bert-base-uncased --pooling mean --lr 5e-5 --use-link-graph --train-path /home/jiangyunqi/KGC/SimKGC/data/WN18RR/train.txt.json --valid-path /home/jiangyunqi/KGC/SimKGC/data/WN18RR/valid.txt.json --task WN18RR --batch-size 1024 --print-freq 20 --additive-margin 0.02 --use-amp --use-self-negative --pre-batch 0 --finetune-t --epochs 50 --workers 4 --max-to-keep 3
[2023-08-24 20:53:36,812 INFO] Load 40943 entities from /home/jiangyunqi/KGC/SimKGC/data/WN18RR/entities.json
[2023-08-24 20:53:36,812 INFO] Triplets path: ['/home/jiangyunqi/KGC/SimKGC/data/WN18RR/train.txt.json']
[2023-08-24 20:53:37,261 INFO] Triplet statistics: 22 relations, 173670 triplets
[2023-08-24 20:53:37,261 INFO] Start to build link graph from /home/jiangyunqi/KGC/SimKGC/data/WN18RR/train.txt.json
[2023-08-24 20:53:37,507 INFO] Done build link graph with 40559 nodes
[2023-08-24 20:53:37,564 INFO] Use 4 gpus for training
[2023-08-24 20:53:38,575 INFO] Build tokenizer from bert-base-uncased
[2023-08-24 20:53:38,576 INFO] => creating model
[2023-08-24 20:53:40,567 INFO] CustomBertModel(
  (hr_bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (tail_bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
)
/home/jiangyunqi/anaconda3/envs/SimKGC/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2023-08-24 20:53:42,428 INFO] module.log_inv_t: 1.0
[2023-08-24 20:53:42,428 INFO] module.hr_bert.embeddings.word_embeddings.weight: 23440896
[2023-08-24 20:53:42,428 INFO] module.hr_bert.embeddings.position_embeddings.weight: 393216
[2023-08-24 20:53:42,428 INFO] module.hr_bert.embeddings.token_type_embeddings.weight: 1536
[2023-08-24 20:53:42,428 INFO] module.hr_bert.embeddings.LayerNorm.weight: 768
[2023-08-24 20:53:42,428 INFO] module.hr_bert.embeddings.LayerNorm.bias: 768
[2023-08-24 20:53:42,429 INFO] module.hr_bert.encoder.layer.0.attention.self.query.weight: 589824
[2023-08-24 20:53:42,429 INFO] module.hr_bert.encoder.layer.0.attention.self.query.bias: 768
[2023-08-24 20:53:42,429 INFO] module.hr_bert.encoder.layer.0.attention.self.key.weight: 589824
[2023-08-24 20:53:42,429 INFO] module.hr_bert.encoder.layer.0.attention.self.key.bias: 768
[2023-08-24 20:53:42,429 INFO] module.hr_bert.encoder.layer.0.attention.self.value.weight: 589824
[2023-08-24 20:53:42,429 INFO] module.hr_bert.encoder.layer.0.attention.self.value.bias: 768
[2023-08-24 20:53:42,429 INFO] module.hr_bert.encoder.layer.0.attention.output.dense.weight: 589824
[2023-08-24 20:53:42,429 INFO] module.hr_bert.encoder.layer.0.attention.output.dense.bias: 768
[2023-08-24 20:53:42,429 INFO] module.hr_bert.encoder.layer.0.attention.output.LayerNorm.weight: 768
[2023-08-24 20:53:42,429 INFO] module.hr_bert.encoder.layer.0.attention.output.LayerNorm.bias: 768
[2023-08-24 20:53:42,429 INFO] module.hr_bert.encoder.layer.0.intermediate.dense.weight: 2359296
[2023-08-24 20:53:42,429 INFO] module.hr_bert.encoder.layer.0.intermediate.dense.bias: 3072
[2023-08-24 20:53:42,429 INFO] module.hr_bert.encoder.layer.0.output.dense.weight: 2359296
[2023-08-24 20:53:42,429 INFO] module.hr_bert.encoder.layer.0.output.dense.bias: 768
[2023-08-24 20:53:42,429 INFO] module.hr_bert.encoder.layer.0.output.LayerNorm.weight: 768
[2023-08-24 20:53:42,429 INFO] module.hr_bert.encoder.layer.0.output.LayerNorm.bias: 768
[2023-08-24 20:53:42,429 INFO] module.hr_bert.encoder.layer.1.attention.self.query.weight: 589824
[2023-08-24 20:53:42,429 INFO] module.hr_bert.encoder.layer.1.attention.self.query.bias: 768
[2023-08-24 20:53:42,429 INFO] module.hr_bert.encoder.layer.1.attention.self.key.weight: 589824
[2023-08-24 20:53:42,429 INFO] module.hr_bert.encoder.layer.1.attention.self.key.bias: 768
[2023-08-24 20:53:42,429 INFO] module.hr_bert.encoder.layer.1.attention.self.value.weight: 589824
[2023-08-24 20:53:42,429 INFO] module.hr_bert.encoder.layer.1.attention.self.value.bias: 768
[2023-08-24 20:53:42,429 INFO] module.hr_bert.encoder.layer.1.attention.output.dense.weight: 589824
[2023-08-24 20:53:42,429 INFO] module.hr_bert.encoder.layer.1.attention.output.dense.bias: 768
[2023-08-24 20:53:42,429 INFO] module.hr_bert.encoder.layer.1.attention.output.LayerNorm.weight: 768
[2023-08-24 20:53:42,429 INFO] module.hr_bert.encoder.layer.1.attention.output.LayerNorm.bias: 768
[2023-08-24 20:53:42,429 INFO] module.hr_bert.encoder.layer.1.intermediate.dense.weight: 2359296
[2023-08-24 20:53:42,430 INFO] module.hr_bert.encoder.layer.1.intermediate.dense.bias: 3072
[2023-08-24 20:53:42,430 INFO] module.hr_bert.encoder.layer.1.output.dense.weight: 2359296
[2023-08-24 20:53:42,430 INFO] module.hr_bert.encoder.layer.1.output.dense.bias: 768
[2023-08-24 20:53:42,430 INFO] module.hr_bert.encoder.layer.1.output.LayerNorm.weight: 768
[2023-08-24 20:53:42,430 INFO] module.hr_bert.encoder.layer.1.output.LayerNorm.bias: 768
[2023-08-24 20:53:42,430 INFO] module.hr_bert.encoder.layer.2.attention.self.query.weight: 589824
[2023-08-24 20:53:42,430 INFO] module.hr_bert.encoder.layer.2.attention.self.query.bias: 768
[2023-08-24 20:53:42,430 INFO] module.hr_bert.encoder.layer.2.attention.self.key.weight: 589824
[2023-08-24 20:53:42,430 INFO] module.hr_bert.encoder.layer.2.attention.self.key.bias: 768
[2023-08-24 20:53:42,430 INFO] module.hr_bert.encoder.layer.2.attention.self.value.weight: 589824
[2023-08-24 20:53:42,430 INFO] module.hr_bert.encoder.layer.2.attention.self.value.bias: 768
[2023-08-24 20:53:42,430 INFO] module.hr_bert.encoder.layer.2.attention.output.dense.weight: 589824
[2023-08-24 20:53:42,430 INFO] module.hr_bert.encoder.layer.2.attention.output.dense.bias: 768
[2023-08-24 20:53:42,430 INFO] module.hr_bert.encoder.layer.2.attention.output.LayerNorm.weight: 768
[2023-08-24 20:53:42,430 INFO] module.hr_bert.encoder.layer.2.attention.output.LayerNorm.bias: 768
[2023-08-24 20:53:42,430 INFO] module.hr_bert.encoder.layer.2.intermediate.dense.weight: 2359296
[2023-08-24 20:53:42,430 INFO] module.hr_bert.encoder.layer.2.intermediate.dense.bias: 3072
[2023-08-24 20:53:42,430 INFO] module.hr_bert.encoder.layer.2.output.dense.weight: 2359296
[2023-08-24 20:53:42,430 INFO] module.hr_bert.encoder.layer.2.output.dense.bias: 768
[2023-08-24 20:53:42,430 INFO] module.hr_bert.encoder.layer.2.output.LayerNorm.weight: 768
[2023-08-24 20:53:42,430 INFO] module.hr_bert.encoder.layer.2.output.LayerNorm.bias: 768
[2023-08-24 20:53:42,430 INFO] module.hr_bert.encoder.layer.3.attention.self.query.weight: 589824
[2023-08-24 20:53:42,430 INFO] module.hr_bert.encoder.layer.3.attention.self.query.bias: 768
[2023-08-24 20:53:42,430 INFO] module.hr_bert.encoder.layer.3.attention.self.key.weight: 589824
[2023-08-24 20:53:42,430 INFO] module.hr_bert.encoder.layer.3.attention.self.key.bias: 768
[2023-08-24 20:53:42,430 INFO] module.hr_bert.encoder.layer.3.attention.self.value.weight: 589824
[2023-08-24 20:53:42,430 INFO] module.hr_bert.encoder.layer.3.attention.self.value.bias: 768
[2023-08-24 20:53:42,431 INFO] module.hr_bert.encoder.layer.3.attention.output.dense.weight: 589824
[2023-08-24 20:53:42,431 INFO] module.hr_bert.encoder.layer.3.attention.output.dense.bias: 768
[2023-08-24 20:53:42,431 INFO] module.hr_bert.encoder.layer.3.attention.output.LayerNorm.weight: 768
[2023-08-24 20:53:42,431 INFO] module.hr_bert.encoder.layer.3.attention.output.LayerNorm.bias: 768
[2023-08-24 20:53:42,431 INFO] module.hr_bert.encoder.layer.3.intermediate.dense.weight: 2359296
[2023-08-24 20:53:42,431 INFO] module.hr_bert.encoder.layer.3.intermediate.dense.bias: 3072
[2023-08-24 20:53:42,431 INFO] module.hr_bert.encoder.layer.3.output.dense.weight: 2359296
[2023-08-24 20:53:42,431 INFO] module.hr_bert.encoder.layer.3.output.dense.bias: 768
[2023-08-24 20:53:42,431 INFO] module.hr_bert.encoder.layer.3.output.LayerNorm.weight: 768
[2023-08-24 20:53:42,431 INFO] module.hr_bert.encoder.layer.3.output.LayerNorm.bias: 768
[2023-08-24 20:53:42,431 INFO] module.hr_bert.encoder.layer.4.attention.self.query.weight: 589824
[2023-08-24 20:53:42,431 INFO] module.hr_bert.encoder.layer.4.attention.self.query.bias: 768
[2023-08-24 20:53:42,431 INFO] module.hr_bert.encoder.layer.4.attention.self.key.weight: 589824
[2023-08-24 20:53:42,431 INFO] module.hr_bert.encoder.layer.4.attention.self.key.bias: 768
[2023-08-24 20:53:42,431 INFO] module.hr_bert.encoder.layer.4.attention.self.value.weight: 589824
[2023-08-24 20:53:42,431 INFO] module.hr_bert.encoder.layer.4.attention.self.value.bias: 768
[2023-08-24 20:53:42,431 INFO] module.hr_bert.encoder.layer.4.attention.output.dense.weight: 589824
[2023-08-24 20:53:42,431 INFO] module.hr_bert.encoder.layer.4.attention.output.dense.bias: 768
[2023-08-24 20:53:42,431 INFO] module.hr_bert.encoder.layer.4.attention.output.LayerNorm.weight: 768
[2023-08-24 20:53:42,431 INFO] module.hr_bert.encoder.layer.4.attention.output.LayerNorm.bias: 768
[2023-08-24 20:53:42,431 INFO] module.hr_bert.encoder.layer.4.intermediate.dense.weight: 2359296
[2023-08-24 20:53:42,431 INFO] module.hr_bert.encoder.layer.4.intermediate.dense.bias: 3072
[2023-08-24 20:53:42,431 INFO] module.hr_bert.encoder.layer.4.output.dense.weight: 2359296
[2023-08-24 20:53:42,431 INFO] module.hr_bert.encoder.layer.4.output.dense.bias: 768
[2023-08-24 20:53:42,431 INFO] module.hr_bert.encoder.layer.4.output.LayerNorm.weight: 768
[2023-08-24 20:53:42,432 INFO] module.hr_bert.encoder.layer.4.output.LayerNorm.bias: 768
[2023-08-24 20:53:42,432 INFO] module.hr_bert.encoder.layer.5.attention.self.query.weight: 589824
[2023-08-24 20:53:42,432 INFO] module.hr_bert.encoder.layer.5.attention.self.query.bias: 768
[2023-08-24 20:53:42,432 INFO] module.hr_bert.encoder.layer.5.attention.self.key.weight: 589824
[2023-08-24 20:53:42,432 INFO] module.hr_bert.encoder.layer.5.attention.self.key.bias: 768
[2023-08-24 20:53:42,432 INFO] module.hr_bert.encoder.layer.5.attention.self.value.weight: 589824
[2023-08-24 20:53:42,432 INFO] module.hr_bert.encoder.layer.5.attention.self.value.bias: 768
[2023-08-24 20:53:42,432 INFO] module.hr_bert.encoder.layer.5.attention.output.dense.weight: 589824
[2023-08-24 20:53:42,432 INFO] module.hr_bert.encoder.layer.5.attention.output.dense.bias: 768
[2023-08-24 20:53:42,432 INFO] module.hr_bert.encoder.layer.5.attention.output.LayerNorm.weight: 768
[2023-08-24 20:53:42,432 INFO] module.hr_bert.encoder.layer.5.attention.output.LayerNorm.bias: 768
[2023-08-24 20:53:42,432 INFO] module.hr_bert.encoder.layer.5.intermediate.dense.weight: 2359296
[2023-08-24 20:53:42,432 INFO] module.hr_bert.encoder.layer.5.intermediate.dense.bias: 3072
[2023-08-24 20:53:42,432 INFO] module.hr_bert.encoder.layer.5.output.dense.weight: 2359296
[2023-08-24 20:53:42,432 INFO] module.hr_bert.encoder.layer.5.output.dense.bias: 768
[2023-08-24 20:53:42,432 INFO] module.hr_bert.encoder.layer.5.output.LayerNorm.weight: 768
[2023-08-24 20:53:42,432 INFO] module.hr_bert.encoder.layer.5.output.LayerNorm.bias: 768
[2023-08-24 20:53:42,432 INFO] module.hr_bert.encoder.layer.6.attention.self.query.weight: 589824
[2023-08-24 20:53:42,432 INFO] module.hr_bert.encoder.layer.6.attention.self.query.bias: 768
[2023-08-24 20:53:42,432 INFO] module.hr_bert.encoder.layer.6.attention.self.key.weight: 589824
[2023-08-24 20:53:42,432 INFO] module.hr_bert.encoder.layer.6.attention.self.key.bias: 768
[2023-08-24 20:53:42,432 INFO] module.hr_bert.encoder.layer.6.attention.self.value.weight: 589824
[2023-08-24 20:53:42,432 INFO] module.hr_bert.encoder.layer.6.attention.self.value.bias: 768
[2023-08-24 20:53:42,432 INFO] module.hr_bert.encoder.layer.6.attention.output.dense.weight: 589824
[2023-08-24 20:53:42,432 INFO] module.hr_bert.encoder.layer.6.attention.output.dense.bias: 768
[2023-08-24 20:53:42,433 INFO] module.hr_bert.encoder.layer.6.attention.output.LayerNorm.weight: 768
[2023-08-24 20:53:42,433 INFO] module.hr_bert.encoder.layer.6.attention.output.LayerNorm.bias: 768
[2023-08-24 20:53:42,433 INFO] module.hr_bert.encoder.layer.6.intermediate.dense.weight: 2359296
[2023-08-24 20:53:42,433 INFO] module.hr_bert.encoder.layer.6.intermediate.dense.bias: 3072
[2023-08-24 20:53:42,433 INFO] module.hr_bert.encoder.layer.6.output.dense.weight: 2359296
[2023-08-24 20:53:42,433 INFO] module.hr_bert.encoder.layer.6.output.dense.bias: 768
[2023-08-24 20:53:42,433 INFO] module.hr_bert.encoder.layer.6.output.LayerNorm.weight: 768
[2023-08-24 20:53:42,433 INFO] module.hr_bert.encoder.layer.6.output.LayerNorm.bias: 768
[2023-08-24 20:53:42,433 INFO] module.hr_bert.encoder.layer.7.attention.self.query.weight: 589824
[2023-08-24 20:53:42,433 INFO] module.hr_bert.encoder.layer.7.attention.self.query.bias: 768
[2023-08-24 20:53:42,433 INFO] module.hr_bert.encoder.layer.7.attention.self.key.weight: 589824
[2023-08-24 20:53:42,433 INFO] module.hr_bert.encoder.layer.7.attention.self.key.bias: 768
[2023-08-24 20:53:42,433 INFO] module.hr_bert.encoder.layer.7.attention.self.value.weight: 589824
[2023-08-24 20:53:42,433 INFO] module.hr_bert.encoder.layer.7.attention.self.value.bias: 768
[2023-08-24 20:53:42,433 INFO] module.hr_bert.encoder.layer.7.attention.output.dense.weight: 589824
[2023-08-24 20:53:42,433 INFO] module.hr_bert.encoder.layer.7.attention.output.dense.bias: 768
[2023-08-24 20:53:42,433 INFO] module.hr_bert.encoder.layer.7.attention.output.LayerNorm.weight: 768
[2023-08-24 20:53:42,433 INFO] module.hr_bert.encoder.layer.7.attention.output.LayerNorm.bias: 768
[2023-08-24 20:53:42,433 INFO] module.hr_bert.encoder.layer.7.intermediate.dense.weight: 2359296
[2023-08-24 20:53:42,433 INFO] module.hr_bert.encoder.layer.7.intermediate.dense.bias: 3072
[2023-08-24 20:53:42,433 INFO] module.hr_bert.encoder.layer.7.output.dense.weight: 2359296
[2023-08-24 20:53:42,433 INFO] module.hr_bert.encoder.layer.7.output.dense.bias: 768
[2023-08-24 20:53:42,433 INFO] module.hr_bert.encoder.layer.7.output.LayerNorm.weight: 768
[2023-08-24 20:53:42,433 INFO] module.hr_bert.encoder.layer.7.output.LayerNorm.bias: 768
[2023-08-24 20:53:42,433 INFO] module.hr_bert.encoder.layer.8.attention.self.query.weight: 589824
[2023-08-24 20:53:42,433 INFO] module.hr_bert.encoder.layer.8.attention.self.query.bias: 768
[2023-08-24 20:53:42,433 INFO] module.hr_bert.encoder.layer.8.attention.self.key.weight: 589824
[2023-08-24 20:53:42,433 INFO] module.hr_bert.encoder.layer.8.attention.self.key.bias: 768
[2023-08-24 20:53:42,434 INFO] module.hr_bert.encoder.layer.8.attention.self.value.weight: 589824
[2023-08-24 20:53:42,434 INFO] module.hr_bert.encoder.layer.8.attention.self.value.bias: 768
[2023-08-24 20:53:42,434 INFO] module.hr_bert.encoder.layer.8.attention.output.dense.weight: 589824
[2023-08-24 20:53:42,434 INFO] module.hr_bert.encoder.layer.8.attention.output.dense.bias: 768
[2023-08-24 20:53:42,434 INFO] module.hr_bert.encoder.layer.8.attention.output.LayerNorm.weight: 768
[2023-08-24 20:53:42,434 INFO] module.hr_bert.encoder.layer.8.attention.output.LayerNorm.bias: 768
[2023-08-24 20:53:42,434 INFO] module.hr_bert.encoder.layer.8.intermediate.dense.weight: 2359296
[2023-08-24 20:53:42,434 INFO] module.hr_bert.encoder.layer.8.intermediate.dense.bias: 3072
[2023-08-24 20:53:42,434 INFO] module.hr_bert.encoder.layer.8.output.dense.weight: 2359296
[2023-08-24 20:53:42,434 INFO] module.hr_bert.encoder.layer.8.output.dense.bias: 768
[2023-08-24 20:53:42,434 INFO] module.hr_bert.encoder.layer.8.output.LayerNorm.weight: 768
[2023-08-24 20:53:42,434 INFO] module.hr_bert.encoder.layer.8.output.LayerNorm.bias: 768
[2023-08-24 20:53:42,434 INFO] module.hr_bert.encoder.layer.9.attention.self.query.weight: 589824
[2023-08-24 20:53:42,434 INFO] module.hr_bert.encoder.layer.9.attention.self.query.bias: 768
[2023-08-24 20:53:42,434 INFO] module.hr_bert.encoder.layer.9.attention.self.key.weight: 589824
[2023-08-24 20:53:42,434 INFO] module.hr_bert.encoder.layer.9.attention.self.key.bias: 768
[2023-08-24 20:53:42,434 INFO] module.hr_bert.encoder.layer.9.attention.self.value.weight: 589824
[2023-08-24 20:53:42,434 INFO] module.hr_bert.encoder.layer.9.attention.self.value.bias: 768
[2023-08-24 20:53:42,434 INFO] module.hr_bert.encoder.layer.9.attention.output.dense.weight: 589824
[2023-08-24 20:53:42,434 INFO] module.hr_bert.encoder.layer.9.attention.output.dense.bias: 768
[2023-08-24 20:53:42,434 INFO] module.hr_bert.encoder.layer.9.attention.output.LayerNorm.weight: 768
[2023-08-24 20:53:42,434 INFO] module.hr_bert.encoder.layer.9.attention.output.LayerNorm.bias: 768
[2023-08-24 20:53:42,434 INFO] module.hr_bert.encoder.layer.9.intermediate.dense.weight: 2359296
[2023-08-24 20:53:42,434 INFO] module.hr_bert.encoder.layer.9.intermediate.dense.bias: 3072
[2023-08-24 20:53:42,434 INFO] module.hr_bert.encoder.layer.9.output.dense.weight: 2359296
[2023-08-24 20:53:42,434 INFO] module.hr_bert.encoder.layer.9.output.dense.bias: 768
[2023-08-24 20:53:42,434 INFO] module.hr_bert.encoder.layer.9.output.LayerNorm.weight: 768
[2023-08-24 20:53:42,435 INFO] module.hr_bert.encoder.layer.9.output.LayerNorm.bias: 768
[2023-08-24 20:53:42,435 INFO] module.hr_bert.encoder.layer.10.attention.self.query.weight: 589824
[2023-08-24 20:53:42,435 INFO] module.hr_bert.encoder.layer.10.attention.self.query.bias: 768
[2023-08-24 20:53:42,435 INFO] module.hr_bert.encoder.layer.10.attention.self.key.weight: 589824
[2023-08-24 20:53:42,435 INFO] module.hr_bert.encoder.layer.10.attention.self.key.bias: 768
[2023-08-24 20:53:42,435 INFO] module.hr_bert.encoder.layer.10.attention.self.value.weight: 589824
[2023-08-24 20:53:42,435 INFO] module.hr_bert.encoder.layer.10.attention.self.value.bias: 768
[2023-08-24 20:53:42,435 INFO] module.hr_bert.encoder.layer.10.attention.output.dense.weight: 589824
[2023-08-24 20:53:42,435 INFO] module.hr_bert.encoder.layer.10.attention.output.dense.bias: 768
[2023-08-24 20:53:42,435 INFO] module.hr_bert.encoder.layer.10.attention.output.LayerNorm.weight: 768
[2023-08-24 20:53:42,435 INFO] module.hr_bert.encoder.layer.10.attention.output.LayerNorm.bias: 768
[2023-08-24 20:53:42,435 INFO] module.hr_bert.encoder.layer.10.intermediate.dense.weight: 2359296
[2023-08-24 20:53:42,435 INFO] module.hr_bert.encoder.layer.10.intermediate.dense.bias: 3072
[2023-08-24 20:53:42,435 INFO] module.hr_bert.encoder.layer.10.output.dense.weight: 2359296
[2023-08-24 20:53:42,435 INFO] module.hr_bert.encoder.layer.10.output.dense.bias: 768
[2023-08-24 20:53:42,435 INFO] module.hr_bert.encoder.layer.10.output.LayerNorm.weight: 768
[2023-08-24 20:53:42,435 INFO] module.hr_bert.encoder.layer.10.output.LayerNorm.bias: 768
[2023-08-24 20:53:42,435 INFO] module.hr_bert.encoder.layer.11.attention.self.query.weight: 589824
[2023-08-24 20:53:42,435 INFO] module.hr_bert.encoder.layer.11.attention.self.query.bias: 768
[2023-08-24 20:53:42,435 INFO] module.hr_bert.encoder.layer.11.attention.self.key.weight: 589824
[2023-08-24 20:53:42,435 INFO] module.hr_bert.encoder.layer.11.attention.self.key.bias: 768
[2023-08-24 20:53:42,435 INFO] module.hr_bert.encoder.layer.11.attention.self.value.weight: 589824
[2023-08-24 20:53:42,435 INFO] module.hr_bert.encoder.layer.11.attention.self.value.bias: 768
[2023-08-24 20:53:42,435 INFO] module.hr_bert.encoder.layer.11.attention.output.dense.weight: 589824
[2023-08-24 20:53:42,435 INFO] module.hr_bert.encoder.layer.11.attention.output.dense.bias: 768
[2023-08-24 20:53:42,435 INFO] module.hr_bert.encoder.layer.11.attention.output.LayerNorm.weight: 768
[2023-08-24 20:53:42,435 INFO] module.hr_bert.encoder.layer.11.attention.output.LayerNorm.bias: 768
[2023-08-24 20:53:42,436 INFO] module.hr_bert.encoder.layer.11.intermediate.dense.weight: 2359296
[2023-08-24 20:53:42,436 INFO] module.hr_bert.encoder.layer.11.intermediate.dense.bias: 3072
[2023-08-24 20:53:42,436 INFO] module.hr_bert.encoder.layer.11.output.dense.weight: 2359296
[2023-08-24 20:53:42,436 INFO] module.hr_bert.encoder.layer.11.output.dense.bias: 768
[2023-08-24 20:53:42,436 INFO] module.hr_bert.encoder.layer.11.output.LayerNorm.weight: 768
[2023-08-24 20:53:42,436 INFO] module.hr_bert.encoder.layer.11.output.LayerNorm.bias: 768
[2023-08-24 20:53:42,436 INFO] module.hr_bert.pooler.dense.weight: 589824
[2023-08-24 20:53:42,436 INFO] module.hr_bert.pooler.dense.bias: 768
[2023-08-24 20:53:42,436 INFO] module.tail_bert.embeddings.word_embeddings.weight: 23440896
[2023-08-24 20:53:42,436 INFO] module.tail_bert.embeddings.position_embeddings.weight: 393216
[2023-08-24 20:53:42,436 INFO] module.tail_bert.embeddings.token_type_embeddings.weight: 1536
[2023-08-24 20:53:42,436 INFO] module.tail_bert.embeddings.LayerNorm.weight: 768
[2023-08-24 20:53:42,436 INFO] module.tail_bert.embeddings.LayerNorm.bias: 768
[2023-08-24 20:53:42,436 INFO] module.tail_bert.encoder.layer.0.attention.self.query.weight: 589824
[2023-08-24 20:53:42,436 INFO] module.tail_bert.encoder.layer.0.attention.self.query.bias: 768
[2023-08-24 20:53:42,436 INFO] module.tail_bert.encoder.layer.0.attention.self.key.weight: 589824
[2023-08-24 20:53:42,436 INFO] module.tail_bert.encoder.layer.0.attention.self.key.bias: 768
[2023-08-24 20:53:42,436 INFO] module.tail_bert.encoder.layer.0.attention.self.value.weight: 589824
[2023-08-24 20:53:42,436 INFO] module.tail_bert.encoder.layer.0.attention.self.value.bias: 768
[2023-08-24 20:53:42,436 INFO] module.tail_bert.encoder.layer.0.attention.output.dense.weight: 589824
[2023-08-24 20:53:42,436 INFO] module.tail_bert.encoder.layer.0.attention.output.dense.bias: 768
[2023-08-24 20:53:42,436 INFO] module.tail_bert.encoder.layer.0.attention.output.LayerNorm.weight: 768
[2023-08-24 20:53:42,436 INFO] module.tail_bert.encoder.layer.0.attention.output.LayerNorm.bias: 768
[2023-08-24 20:53:42,436 INFO] module.tail_bert.encoder.layer.0.intermediate.dense.weight: 2359296
[2023-08-24 20:53:42,436 INFO] module.tail_bert.encoder.layer.0.intermediate.dense.bias: 3072
[2023-08-24 20:53:42,436 INFO] module.tail_bert.encoder.layer.0.output.dense.weight: 2359296
[2023-08-24 20:53:42,436 INFO] module.tail_bert.encoder.layer.0.output.dense.bias: 768
[2023-08-24 20:53:42,437 INFO] module.tail_bert.encoder.layer.0.output.LayerNorm.weight: 768
[2023-08-24 20:53:42,437 INFO] module.tail_bert.encoder.layer.0.output.LayerNorm.bias: 768
[2023-08-24 20:53:42,437 INFO] module.tail_bert.encoder.layer.1.attention.self.query.weight: 589824
[2023-08-24 20:53:42,437 INFO] module.tail_bert.encoder.layer.1.attention.self.query.bias: 768
[2023-08-24 20:53:42,437 INFO] module.tail_bert.encoder.layer.1.attention.self.key.weight: 589824
[2023-08-24 20:53:42,437 INFO] module.tail_bert.encoder.layer.1.attention.self.key.bias: 768
[2023-08-24 20:53:42,437 INFO] module.tail_bert.encoder.layer.1.attention.self.value.weight: 589824
[2023-08-24 20:53:42,437 INFO] module.tail_bert.encoder.layer.1.attention.self.value.bias: 768
[2023-08-24 20:53:42,437 INFO] module.tail_bert.encoder.layer.1.attention.output.dense.weight: 589824
[2023-08-24 20:53:42,437 INFO] module.tail_bert.encoder.layer.1.attention.output.dense.bias: 768
[2023-08-24 20:53:42,437 INFO] module.tail_bert.encoder.layer.1.attention.output.LayerNorm.weight: 768
[2023-08-24 20:53:42,437 INFO] module.tail_bert.encoder.layer.1.attention.output.LayerNorm.bias: 768
[2023-08-24 20:53:42,437 INFO] module.tail_bert.encoder.layer.1.intermediate.dense.weight: 2359296
[2023-08-24 20:53:42,437 INFO] module.tail_bert.encoder.layer.1.intermediate.dense.bias: 3072
[2023-08-24 20:53:42,437 INFO] module.tail_bert.encoder.layer.1.output.dense.weight: 2359296
[2023-08-24 20:53:42,437 INFO] module.tail_bert.encoder.layer.1.output.dense.bias: 768
[2023-08-24 20:53:42,437 INFO] module.tail_bert.encoder.layer.1.output.LayerNorm.weight: 768
[2023-08-24 20:53:42,437 INFO] module.tail_bert.encoder.layer.1.output.LayerNorm.bias: 768
[2023-08-24 20:53:42,437 INFO] module.tail_bert.encoder.layer.2.attention.self.query.weight: 589824
[2023-08-24 20:53:42,437 INFO] module.tail_bert.encoder.layer.2.attention.self.query.bias: 768
[2023-08-24 20:53:42,437 INFO] module.tail_bert.encoder.layer.2.attention.self.key.weight: 589824
[2023-08-24 20:53:42,437 INFO] module.tail_bert.encoder.layer.2.attention.self.key.bias: 768
[2023-08-24 20:53:42,437 INFO] module.tail_bert.encoder.layer.2.attention.self.value.weight: 589824
[2023-08-24 20:53:42,437 INFO] module.tail_bert.encoder.layer.2.attention.self.value.bias: 768
[2023-08-24 20:53:42,437 INFO] module.tail_bert.encoder.layer.2.attention.output.dense.weight: 589824
[2023-08-24 20:53:42,437 INFO] module.tail_bert.encoder.layer.2.attention.output.dense.bias: 768
[2023-08-24 20:53:42,437 INFO] module.tail_bert.encoder.layer.2.attention.output.LayerNorm.weight: 768
[2023-08-24 20:53:42,437 INFO] module.tail_bert.encoder.layer.2.attention.output.LayerNorm.bias: 768
[2023-08-24 20:53:42,438 INFO] module.tail_bert.encoder.layer.2.intermediate.dense.weight: 2359296
[2023-08-24 20:53:42,438 INFO] module.tail_bert.encoder.layer.2.intermediate.dense.bias: 3072
[2023-08-24 20:53:42,438 INFO] module.tail_bert.encoder.layer.2.output.dense.weight: 2359296
[2023-08-24 20:53:42,438 INFO] module.tail_bert.encoder.layer.2.output.dense.bias: 768
[2023-08-24 20:53:42,438 INFO] module.tail_bert.encoder.layer.2.output.LayerNorm.weight: 768
[2023-08-24 20:53:42,438 INFO] module.tail_bert.encoder.layer.2.output.LayerNorm.bias: 768
[2023-08-24 20:53:42,438 INFO] module.tail_bert.encoder.layer.3.attention.self.query.weight: 589824
[2023-08-24 20:53:42,438 INFO] module.tail_bert.encoder.layer.3.attention.self.query.bias: 768
[2023-08-24 20:53:42,438 INFO] module.tail_bert.encoder.layer.3.attention.self.key.weight: 589824
[2023-08-24 20:53:42,438 INFO] module.tail_bert.encoder.layer.3.attention.self.key.bias: 768
[2023-08-24 20:53:42,438 INFO] module.tail_bert.encoder.layer.3.attention.self.value.weight: 589824
[2023-08-24 20:53:42,438 INFO] module.tail_bert.encoder.layer.3.attention.self.value.bias: 768
[2023-08-24 20:53:42,438 INFO] module.tail_bert.encoder.layer.3.attention.output.dense.weight: 589824
[2023-08-24 20:53:42,438 INFO] module.tail_bert.encoder.layer.3.attention.output.dense.bias: 768
[2023-08-24 20:53:42,438 INFO] module.tail_bert.encoder.layer.3.attention.output.LayerNorm.weight: 768
[2023-08-24 20:53:42,438 INFO] module.tail_bert.encoder.layer.3.attention.output.LayerNorm.bias: 768
[2023-08-24 20:53:42,438 INFO] module.tail_bert.encoder.layer.3.intermediate.dense.weight: 2359296
[2023-08-24 20:53:42,438 INFO] module.tail_bert.encoder.layer.3.intermediate.dense.bias: 3072
[2023-08-24 20:53:42,438 INFO] module.tail_bert.encoder.layer.3.output.dense.weight: 2359296
[2023-08-24 20:53:42,438 INFO] module.tail_bert.encoder.layer.3.output.dense.bias: 768
[2023-08-24 20:53:42,438 INFO] module.tail_bert.encoder.layer.3.output.LayerNorm.weight: 768
[2023-08-24 20:53:42,438 INFO] module.tail_bert.encoder.layer.3.output.LayerNorm.bias: 768
[2023-08-24 20:53:42,438 INFO] module.tail_bert.encoder.layer.4.attention.self.query.weight: 589824
[2023-08-24 20:53:42,438 INFO] module.tail_bert.encoder.layer.4.attention.self.query.bias: 768
[2023-08-24 20:53:42,438 INFO] module.tail_bert.encoder.layer.4.attention.self.key.weight: 589824
[2023-08-24 20:53:42,438 INFO] module.tail_bert.encoder.layer.4.attention.self.key.bias: 768
[2023-08-24 20:53:42,438 INFO] module.tail_bert.encoder.layer.4.attention.self.value.weight: 589824
[2023-08-24 20:53:42,439 INFO] module.tail_bert.encoder.layer.4.attention.self.value.bias: 768
[2023-08-24 20:53:42,439 INFO] module.tail_bert.encoder.layer.4.attention.output.dense.weight: 589824
[2023-08-24 20:53:42,439 INFO] module.tail_bert.encoder.layer.4.attention.output.dense.bias: 768
[2023-08-24 20:53:42,439 INFO] module.tail_bert.encoder.layer.4.attention.output.LayerNorm.weight: 768
[2023-08-24 20:53:42,439 INFO] module.tail_bert.encoder.layer.4.attention.output.LayerNorm.bias: 768
[2023-08-24 20:53:42,439 INFO] module.tail_bert.encoder.layer.4.intermediate.dense.weight: 2359296
[2023-08-24 20:53:42,439 INFO] module.tail_bert.encoder.layer.4.intermediate.dense.bias: 3072
[2023-08-24 20:53:42,439 INFO] module.tail_bert.encoder.layer.4.output.dense.weight: 2359296
[2023-08-24 20:53:42,439 INFO] module.tail_bert.encoder.layer.4.output.dense.bias: 768
[2023-08-24 20:53:42,439 INFO] module.tail_bert.encoder.layer.4.output.LayerNorm.weight: 768
[2023-08-24 20:53:42,439 INFO] module.tail_bert.encoder.layer.4.output.LayerNorm.bias: 768
[2023-08-24 20:53:42,439 INFO] module.tail_bert.encoder.layer.5.attention.self.query.weight: 589824
[2023-08-24 20:53:42,439 INFO] module.tail_bert.encoder.layer.5.attention.self.query.bias: 768
[2023-08-24 20:53:42,439 INFO] module.tail_bert.encoder.layer.5.attention.self.key.weight: 589824
[2023-08-24 20:53:42,439 INFO] module.tail_bert.encoder.layer.5.attention.self.key.bias: 768
[2023-08-24 20:53:42,439 INFO] module.tail_bert.encoder.layer.5.attention.self.value.weight: 589824
[2023-08-24 20:53:42,439 INFO] module.tail_bert.encoder.layer.5.attention.self.value.bias: 768
[2023-08-24 20:53:42,439 INFO] module.tail_bert.encoder.layer.5.attention.output.dense.weight: 589824
[2023-08-24 20:53:42,439 INFO] module.tail_bert.encoder.layer.5.attention.output.dense.bias: 768
[2023-08-24 20:53:42,439 INFO] module.tail_bert.encoder.layer.5.attention.output.LayerNorm.weight: 768
[2023-08-24 20:53:42,439 INFO] module.tail_bert.encoder.layer.5.attention.output.LayerNorm.bias: 768
[2023-08-24 20:53:42,439 INFO] module.tail_bert.encoder.layer.5.intermediate.dense.weight: 2359296
[2023-08-24 20:53:42,439 INFO] module.tail_bert.encoder.layer.5.intermediate.dense.bias: 3072
[2023-08-24 20:53:42,439 INFO] module.tail_bert.encoder.layer.5.output.dense.weight: 2359296
[2023-08-24 20:53:42,439 INFO] module.tail_bert.encoder.layer.5.output.dense.bias: 768
[2023-08-24 20:53:42,439 INFO] module.tail_bert.encoder.layer.5.output.LayerNorm.weight: 768
[2023-08-24 20:53:42,439 INFO] module.tail_bert.encoder.layer.5.output.LayerNorm.bias: 768
[2023-08-24 20:53:42,440 INFO] module.tail_bert.encoder.layer.6.attention.self.query.weight: 589824
[2023-08-24 20:53:42,440 INFO] module.tail_bert.encoder.layer.6.attention.self.query.bias: 768
[2023-08-24 20:53:42,440 INFO] module.tail_bert.encoder.layer.6.attention.self.key.weight: 589824
[2023-08-24 20:53:42,440 INFO] module.tail_bert.encoder.layer.6.attention.self.key.bias: 768
[2023-08-24 20:53:42,440 INFO] module.tail_bert.encoder.layer.6.attention.self.value.weight: 589824
[2023-08-24 20:53:42,440 INFO] module.tail_bert.encoder.layer.6.attention.self.value.bias: 768
[2023-08-24 20:53:42,440 INFO] module.tail_bert.encoder.layer.6.attention.output.dense.weight: 589824
[2023-08-24 20:53:42,440 INFO] module.tail_bert.encoder.layer.6.attention.output.dense.bias: 768
[2023-08-24 20:53:42,440 INFO] module.tail_bert.encoder.layer.6.attention.output.LayerNorm.weight: 768
[2023-08-24 20:53:42,440 INFO] module.tail_bert.encoder.layer.6.attention.output.LayerNorm.bias: 768
[2023-08-24 20:53:42,440 INFO] module.tail_bert.encoder.layer.6.intermediate.dense.weight: 2359296
[2023-08-24 20:53:42,440 INFO] module.tail_bert.encoder.layer.6.intermediate.dense.bias: 3072
[2023-08-24 20:53:42,440 INFO] module.tail_bert.encoder.layer.6.output.dense.weight: 2359296
[2023-08-24 20:53:42,440 INFO] module.tail_bert.encoder.layer.6.output.dense.bias: 768
[2023-08-24 20:53:42,440 INFO] module.tail_bert.encoder.layer.6.output.LayerNorm.weight: 768
[2023-08-24 20:53:42,440 INFO] module.tail_bert.encoder.layer.6.output.LayerNorm.bias: 768
[2023-08-24 20:53:42,440 INFO] module.tail_bert.encoder.layer.7.attention.self.query.weight: 589824
[2023-08-24 20:53:42,440 INFO] module.tail_bert.encoder.layer.7.attention.self.query.bias: 768
[2023-08-24 20:53:42,440 INFO] module.tail_bert.encoder.layer.7.attention.self.key.weight: 589824
[2023-08-24 20:53:42,440 INFO] module.tail_bert.encoder.layer.7.attention.self.key.bias: 768
[2023-08-24 20:53:42,440 INFO] module.tail_bert.encoder.layer.7.attention.self.value.weight: 589824
[2023-08-24 20:53:42,440 INFO] module.tail_bert.encoder.layer.7.attention.self.value.bias: 768
[2023-08-24 20:53:42,440 INFO] module.tail_bert.encoder.layer.7.attention.output.dense.weight: 589824
[2023-08-24 20:53:42,440 INFO] module.tail_bert.encoder.layer.7.attention.output.dense.bias: 768
[2023-08-24 20:53:42,440 INFO] module.tail_bert.encoder.layer.7.attention.output.LayerNorm.weight: 768
[2023-08-24 20:53:42,440 INFO] module.tail_bert.encoder.layer.7.attention.output.LayerNorm.bias: 768
[2023-08-24 20:53:42,440 INFO] module.tail_bert.encoder.layer.7.intermediate.dense.weight: 2359296
[2023-08-24 20:53:42,440 INFO] module.tail_bert.encoder.layer.7.intermediate.dense.bias: 3072
[2023-08-24 20:53:42,441 INFO] module.tail_bert.encoder.layer.7.output.dense.weight: 2359296
[2023-08-24 20:53:42,441 INFO] module.tail_bert.encoder.layer.7.output.dense.bias: 768
[2023-08-24 20:53:42,441 INFO] module.tail_bert.encoder.layer.7.output.LayerNorm.weight: 768
[2023-08-24 20:53:42,441 INFO] module.tail_bert.encoder.layer.7.output.LayerNorm.bias: 768
[2023-08-24 20:53:42,441 INFO] module.tail_bert.encoder.layer.8.attention.self.query.weight: 589824
[2023-08-24 20:53:42,441 INFO] module.tail_bert.encoder.layer.8.attention.self.query.bias: 768
[2023-08-24 20:53:42,441 INFO] module.tail_bert.encoder.layer.8.attention.self.key.weight: 589824
[2023-08-24 20:53:42,441 INFO] module.tail_bert.encoder.layer.8.attention.self.key.bias: 768
[2023-08-24 20:53:42,441 INFO] module.tail_bert.encoder.layer.8.attention.self.value.weight: 589824
[2023-08-24 20:53:42,441 INFO] module.tail_bert.encoder.layer.8.attention.self.value.bias: 768
[2023-08-24 20:53:42,441 INFO] module.tail_bert.encoder.layer.8.attention.output.dense.weight: 589824
[2023-08-24 20:53:42,441 INFO] module.tail_bert.encoder.layer.8.attention.output.dense.bias: 768
[2023-08-24 20:53:42,441 INFO] module.tail_bert.encoder.layer.8.attention.output.LayerNorm.weight: 768
[2023-08-24 20:53:42,441 INFO] module.tail_bert.encoder.layer.8.attention.output.LayerNorm.bias: 768
[2023-08-24 20:53:42,441 INFO] module.tail_bert.encoder.layer.8.intermediate.dense.weight: 2359296
[2023-08-24 20:53:42,441 INFO] module.tail_bert.encoder.layer.8.intermediate.dense.bias: 3072
[2023-08-24 20:53:42,441 INFO] module.tail_bert.encoder.layer.8.output.dense.weight: 2359296
[2023-08-24 20:53:42,441 INFO] module.tail_bert.encoder.layer.8.output.dense.bias: 768
[2023-08-24 20:53:42,441 INFO] module.tail_bert.encoder.layer.8.output.LayerNorm.weight: 768
[2023-08-24 20:53:42,441 INFO] module.tail_bert.encoder.layer.8.output.LayerNorm.bias: 768
[2023-08-24 20:53:42,441 INFO] module.tail_bert.encoder.layer.9.attention.self.query.weight: 589824
[2023-08-24 20:53:42,441 INFO] module.tail_bert.encoder.layer.9.attention.self.query.bias: 768
[2023-08-24 20:53:42,441 INFO] module.tail_bert.encoder.layer.9.attention.self.key.weight: 589824
[2023-08-24 20:53:42,441 INFO] module.tail_bert.encoder.layer.9.attention.self.key.bias: 768
[2023-08-24 20:53:42,441 INFO] module.tail_bert.encoder.layer.9.attention.self.value.weight: 589824
[2023-08-24 20:53:42,441 INFO] module.tail_bert.encoder.layer.9.attention.self.value.bias: 768
[2023-08-24 20:53:42,441 INFO] module.tail_bert.encoder.layer.9.attention.output.dense.weight: 589824
[2023-08-24 20:53:42,442 INFO] module.tail_bert.encoder.layer.9.attention.output.dense.bias: 768
[2023-08-24 20:53:42,442 INFO] module.tail_bert.encoder.layer.9.attention.output.LayerNorm.weight: 768
[2023-08-24 20:53:42,442 INFO] module.tail_bert.encoder.layer.9.attention.output.LayerNorm.bias: 768
[2023-08-24 20:53:42,442 INFO] module.tail_bert.encoder.layer.9.intermediate.dense.weight: 2359296
[2023-08-24 20:53:42,442 INFO] module.tail_bert.encoder.layer.9.intermediate.dense.bias: 3072
[2023-08-24 20:53:42,442 INFO] module.tail_bert.encoder.layer.9.output.dense.weight: 2359296
[2023-08-24 20:53:42,442 INFO] module.tail_bert.encoder.layer.9.output.dense.bias: 768
[2023-08-24 20:53:42,442 INFO] module.tail_bert.encoder.layer.9.output.LayerNorm.weight: 768
[2023-08-24 20:53:42,442 INFO] module.tail_bert.encoder.layer.9.output.LayerNorm.bias: 768
[2023-08-24 20:53:42,442 INFO] module.tail_bert.encoder.layer.10.attention.self.query.weight: 589824
[2023-08-24 20:53:42,442 INFO] module.tail_bert.encoder.layer.10.attention.self.query.bias: 768
[2023-08-24 20:53:42,442 INFO] module.tail_bert.encoder.layer.10.attention.self.key.weight: 589824
[2023-08-24 20:53:42,442 INFO] module.tail_bert.encoder.layer.10.attention.self.key.bias: 768
[2023-08-24 20:53:42,442 INFO] module.tail_bert.encoder.layer.10.attention.self.value.weight: 589824
[2023-08-24 20:53:42,442 INFO] module.tail_bert.encoder.layer.10.attention.self.value.bias: 768
[2023-08-24 20:53:42,442 INFO] module.tail_bert.encoder.layer.10.attention.output.dense.weight: 589824
[2023-08-24 20:53:42,442 INFO] module.tail_bert.encoder.layer.10.attention.output.dense.bias: 768
[2023-08-24 20:53:42,442 INFO] module.tail_bert.encoder.layer.10.attention.output.LayerNorm.weight: 768
[2023-08-24 20:53:42,442 INFO] module.tail_bert.encoder.layer.10.attention.output.LayerNorm.bias: 768
[2023-08-24 20:53:42,442 INFO] module.tail_bert.encoder.layer.10.intermediate.dense.weight: 2359296
[2023-08-24 20:53:42,442 INFO] module.tail_bert.encoder.layer.10.intermediate.dense.bias: 3072
[2023-08-24 20:53:42,442 INFO] module.tail_bert.encoder.layer.10.output.dense.weight: 2359296
[2023-08-24 20:53:42,442 INFO] module.tail_bert.encoder.layer.10.output.dense.bias: 768
[2023-08-24 20:53:42,442 INFO] module.tail_bert.encoder.layer.10.output.LayerNorm.weight: 768
[2023-08-24 20:53:42,442 INFO] module.tail_bert.encoder.layer.10.output.LayerNorm.bias: 768
[2023-08-24 20:53:42,442 INFO] module.tail_bert.encoder.layer.11.attention.self.query.weight: 589824
[2023-08-24 20:53:42,442 INFO] module.tail_bert.encoder.layer.11.attention.self.query.bias: 768
[2023-08-24 20:53:42,443 INFO] module.tail_bert.encoder.layer.11.attention.self.key.weight: 589824
[2023-08-24 20:53:42,443 INFO] module.tail_bert.encoder.layer.11.attention.self.key.bias: 768
[2023-08-24 20:53:42,443 INFO] module.tail_bert.encoder.layer.11.attention.self.value.weight: 589824
[2023-08-24 20:53:42,443 INFO] module.tail_bert.encoder.layer.11.attention.self.value.bias: 768
[2023-08-24 20:53:42,443 INFO] module.tail_bert.encoder.layer.11.attention.output.dense.weight: 589824
[2023-08-24 20:53:42,443 INFO] module.tail_bert.encoder.layer.11.attention.output.dense.bias: 768
[2023-08-24 20:53:42,443 INFO] module.tail_bert.encoder.layer.11.attention.output.LayerNorm.weight: 768
[2023-08-24 20:53:42,443 INFO] module.tail_bert.encoder.layer.11.attention.output.LayerNorm.bias: 768
[2023-08-24 20:53:42,443 INFO] module.tail_bert.encoder.layer.11.intermediate.dense.weight: 2359296
[2023-08-24 20:53:42,443 INFO] module.tail_bert.encoder.layer.11.intermediate.dense.bias: 3072
[2023-08-24 20:53:42,443 INFO] module.tail_bert.encoder.layer.11.output.dense.weight: 2359296
[2023-08-24 20:53:42,443 INFO] module.tail_bert.encoder.layer.11.output.dense.bias: 768
[2023-08-24 20:53:42,443 INFO] module.tail_bert.encoder.layer.11.output.LayerNorm.weight: 768
[2023-08-24 20:53:42,443 INFO] module.tail_bert.encoder.layer.11.output.LayerNorm.bias: 768
[2023-08-24 20:53:42,443 INFO] module.tail_bert.pooler.dense.weight: 589824
[2023-08-24 20:53:42,443 INFO] module.tail_bert.pooler.dense.bias: 768
[2023-08-24 20:53:42,443 INFO] Number of parameters: 218.0M
[2023-08-24 20:53:42,443 INFO] In test mode: False
[2023-08-24 20:53:42,532 INFO] Load 86835 examples from /home/jiangyunqi/KGC/SimKGC/data/WN18RR/train.txt.json
[2023-08-24 20:53:42,917 INFO] In test mode: False
[2023-08-24 20:53:42,921 INFO] Load 3034 examples from /home/jiangyunqi/KGC/SimKGC/data/WN18RR/valid.txt.json
[2023-08-24 20:53:42,927 INFO] Total training steps: 8479, warmup steps: 400
/home/jiangyunqi/anaconda3/envs/SimKGC/lib/python3.9/site-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
[2023-08-24 20:53:42,928 INFO] Args={
    "pretrained_model": "bert-base-uncased",
    "task": "WN18RR",
    "train_path": "/home/jiangyunqi/KGC/SimKGC/data/WN18RR/train.txt.json",
    "valid_path": "/home/jiangyunqi/KGC/SimKGC/data/WN18RR/valid.txt.json",
    "model_dir": "./checkpoint/wn18rr/",
    "warmup": 400,
    "max_to_keep": 3,
    "grad_clip": 10.0,
    "pooling": "mean",
    "dropout": 0.1,
    "use_amp": true,
    "t": 0.05,
    "use_link_graph": true,
    "eval_every_n_step": 10000,
    "pre_batch": 0,
    "pre_batch_weight": 0.5,
    "additive_margin": 0.02,
    "finetune_t": true,
    "max_num_tokens": 50,
    "use_self_negative": true,
    "workers": 4,
    "epochs": 50,
    "batch_size": 1024,
    "lr": 5e-05,
    "lr_scheduler": "linear",
    "weight_decay": 0.0001,
    "print_freq": 20,
    "seed": null,
    "is_test": false,
    "rerank_n_hop": 2,
    "neighbor_weight": 0.0,
    "eval_model_path": ""
}
[2023-08-24 20:53:59,377 INFO] Epoch: [0][  0/169]	Loss 10.66 (10.66)	InvT  20.00 ( 20.00)	Acc@1   0.00 (  0.00)	Acc@3  19.34 ( 19.34)
[2023-08-24 20:54:25,293 INFO] Epoch: [0][ 20/169]	Loss 9.887 (10.35)	InvT  20.00 ( 20.00)	Acc@1   0.00 (  0.01)	Acc@3  24.12 ( 20.54)
[2023-08-24 20:55:03,894 INFO] Epoch: [0][ 40/169]	Loss 8.638 (9.799)	InvT  20.00 ( 20.00)	Acc@1   0.00 (  0.01)	Acc@3  35.74 ( 25.55)
[2023-08-24 20:55:35,878 INFO] Epoch: [0][ 60/169]	Loss 7.865 (9.308)	InvT  20.00 ( 20.00)	Acc@1   0.00 (  0.02)	Acc@3  39.45 ( 29.28)
[2023-08-24 20:56:04,539 INFO] Epoch: [0][ 80/169]	Loss 7.448 (8.888)	InvT  19.99 ( 20.00)	Acc@1   0.00 (  0.02)	Acc@3  44.43 ( 32.37)
[2023-08-24 20:56:31,511 INFO] Epoch: [0][100/169]	Loss 6.867 (8.544)	InvT  19.99 ( 20.00)	Acc@1   0.49 (  0.03)	Acc@3  46.58 ( 34.86)
[2023-08-24 20:56:57,730 INFO] Epoch: [0][120/169]	Loss 6.286 (8.215)	InvT  19.99 ( 20.00)	Acc@1   6.84 (  0.51)	Acc@3  54.10 ( 37.34)
[2023-08-24 20:57:26,929 INFO] Epoch: [0][140/169]	Loss 5.624 (7.884)	InvT  19.98 ( 19.99)	Acc@1  14.36 (  1.88)	Acc@3  58.01 ( 40.01)
[2023-08-24 20:57:53,751 INFO] Epoch: [0][160/169]	Loss 5.06 (7.567)	InvT  19.98 ( 19.99)	Acc@1  17.97 (  3.63)	Acc@3  65.43 ( 42.65)
[2023-08-24 20:58:02,841 INFO] Learning rate: 2.1125000000000002e-05
[2023-08-24 20:58:10,732 INFO] Epoch 0, valid metric: {"Acc@1": 40.326, "Acc@3": 66.661, "loss": 2.476}
[2023-08-24 20:58:17,050 INFO] Delete old checkpoint ./checkpoint/wn18rr/checkpoint_epoch2.mdl
[2023-08-24 20:58:22,978 INFO] Epoch: [1][  0/169]	Loss 4.691 (4.691)	InvT  19.98 ( 19.98)	Acc@1  22.17 ( 22.17)	Acc@3  68.07 ( 68.07)
[2023-08-24 20:58:49,795 INFO] Epoch: [1][ 20/169]	Loss 4.571 (4.67)	InvT  19.98 ( 19.98)	Acc@1  25.10 ( 23.51)	Acc@3  66.50 ( 67.03)
[2023-08-24 20:59:14,883 INFO] Epoch: [1][ 40/169]	Loss 4.003 (4.452)	InvT  19.98 ( 19.98)	Acc@1  38.48 ( 28.16)	Acc@3  73.24 ( 68.69)
[2023-08-24 20:59:40,362 INFO] Epoch: [1][ 60/169]	Loss 3.648 (4.217)	InvT  19.98 ( 19.98)	Acc@1  43.95 ( 32.65)	Acc@3  75.39 ( 70.54)
[2023-08-24 21:00:08,064 INFO] Epoch: [1][ 80/169]	Loss 3.433 (4.042)	InvT  19.98 ( 19.98)	Acc@1  49.41 ( 36.04)	Acc@3  76.17 ( 71.84)
[2023-08-24 21:00:33,416 INFO] Epoch: [1][100/169]	Loss 3.171 (3.895)	InvT  19.98 ( 19.98)	Acc@1  53.12 ( 38.67)	Acc@3  80.18 ( 73.03)
[2023-08-24 21:00:58,818 INFO] Epoch: [1][120/169]	Loss 2.944 (3.765)	InvT  19.99 ( 19.98)	Acc@1  57.03 ( 40.94)	Acc@3  81.64 ( 74.07)
[2023-08-24 21:01:24,031 INFO] Epoch: [1][140/169]	Loss 2.948 (3.656)	InvT  19.99 ( 19.98)	Acc@1  58.50 ( 42.86)	Acc@3  81.93 ( 74.94)
[2023-08-24 21:01:51,055 INFO] Epoch: [1][160/169]	Loss 2.781 (3.559)	InvT  20.00 ( 19.98)	Acc@1  55.76 ( 44.41)	Acc@3  83.89 ( 75.72)
[2023-08-24 21:02:00,241 INFO] Learning rate: 4.2250000000000004e-05
[2023-08-24 21:02:08,331 INFO] Epoch 1, valid metric: {"Acc@1": 62.772, "Acc@3": 81.658, "loss": 1.46}
[2023-08-24 21:02:15,215 INFO] Delete old checkpoint ./checkpoint/wn18rr/checkpoint_epoch3.mdl
[2023-08-24 21:02:23,063 INFO] Epoch: [2][  0/169]	Loss 2.395 (2.395)	InvT  20.00 ( 20.00)	Acc@1  61.43 ( 61.43)	Acc@3  84.47 ( 84.47)
[2023-08-24 21:03:01,505 INFO] Epoch: [2][ 20/169]	Loss 2.414 (2.402)	InvT  20.01 ( 20.01)	Acc@1  61.43 ( 61.39)	Acc@3  86.13 ( 85.70)
[2023-08-24 21:03:27,736 INFO] Epoch: [2][ 40/169]	Loss 2.108 (2.38)	InvT  20.02 ( 20.01)	Acc@1  66.41 ( 61.81)	Acc@3  86.62 ( 85.78)
[2023-08-24 21:03:52,756 INFO] Epoch: [2][ 60/169]	Loss 2.385 (2.337)	InvT  20.04 ( 20.02)	Acc@1  63.48 ( 62.64)	Acc@3  85.74 ( 86.06)
[2023-08-24 21:04:18,016 INFO] Epoch: [2][ 80/169]	Loss 2.073 (2.308)	InvT  20.05 ( 20.02)	Acc@1  65.72 ( 63.17)	Acc@3  88.67 ( 86.22)
[2023-08-24 21:04:44,248 INFO] Epoch: [2][100/169]	Loss 2.128 (2.281)	InvT  20.07 ( 20.03)	Acc@1  64.06 ( 63.51)	Acc@3  88.09 ( 86.43)
[2023-08-24 21:05:09,746 INFO] Epoch: [2][120/169]	Loss 2.168 (2.257)	InvT  20.08 ( 20.04)	Acc@1  67.87 ( 63.93)	Acc@3  86.82 ( 86.62)
[2023-08-24 21:05:34,495 INFO] Epoch: [2][140/169]	Loss 2.019 (2.23)	InvT  20.10 ( 20.05)	Acc@1  69.73 ( 64.44)	Acc@3  88.67 ( 86.81)
[2023-08-24 21:06:01,936 INFO] Epoch: [2][160/169]	Loss 2.098 (2.201)	InvT  20.11 ( 20.05)	Acc@1  69.53 ( 64.91)	Acc@3  86.72 ( 87.06)
[2023-08-24 21:06:11,288 INFO] Learning rate: 4.9337789330362674e-05
[2023-08-24 21:06:21,292 INFO] Epoch 2, valid metric: {"Acc@1": 68.161, "Acc@3": 85.564, "loss": 1.218}
[2023-08-24 21:06:27,824 INFO] Delete old checkpoint ./checkpoint/wn18rr/checkpoint_epoch4.mdl
[2023-08-24 21:06:32,527 INFO] Epoch: [3][  0/169]	Loss 1.64 (1.64)	InvT  20.12 ( 20.12)	Acc@1  73.14 ( 73.14)	Acc@3  91.60 ( 91.60)
[2023-08-24 21:06:58,693 INFO] Epoch: [3][ 20/169]	Loss 1.717 (1.571)	InvT  20.14 ( 20.13)	Acc@1  71.00 ( 73.55)	Acc@3  91.11 ( 92.15)
[2023-08-24 21:07:25,808 INFO] Epoch: [3][ 40/169]	Loss 1.626 (1.566)	InvT  20.16 ( 20.14)	Acc@1  72.07 ( 73.70)	Acc@3  91.99 ( 92.26)
[2023-08-24 21:07:56,058 INFO] Epoch: [3][ 60/169]	Loss 1.505 (1.567)	InvT  20.18 ( 20.15)	Acc@1  72.66 ( 73.71)	Acc@3  93.16 ( 92.30)
[2023-08-24 21:08:23,852 INFO] Epoch: [3][ 80/169]	Loss 1.516 (1.555)	InvT  20.20 ( 20.16)	Acc@1  75.88 ( 74.06)	Acc@3  92.48 ( 92.34)
[2023-08-24 21:08:48,988 INFO] Epoch: [3][100/169]	Loss 1.507 (1.552)	InvT  20.22 ( 20.17)	Acc@1  74.51 ( 74.08)	Acc@3  91.99 ( 92.36)
[2023-08-24 21:09:14,198 INFO] Epoch: [3][120/169]	Loss 1.448 (1.549)	InvT  20.24 ( 20.18)	Acc@1  76.95 ( 74.12)	Acc@3  93.16 ( 92.37)
[2023-08-24 21:09:39,606 INFO] Epoch: [3][140/169]	Loss 1.406 (1.543)	InvT  20.26 ( 20.19)	Acc@1  75.88 ( 74.21)	Acc@3  93.46 ( 92.40)
[2023-08-24 21:10:04,868 INFO] Epoch: [3][160/169]	Loss 1.47 (1.538)	InvT  20.28 ( 20.20)	Acc@1  77.25 ( 74.36)	Acc@3  93.65 ( 92.42)
[2023-08-24 21:10:14,053 INFO] Learning rate: 4.8291867805421465e-05
[2023-08-24 21:10:21,849 INFO] Epoch 3, valid metric: {"Acc@1": 70.814, "Acc@3": 87.92, "loss": 1.085}
[2023-08-24 21:10:27,690 INFO] Delete old checkpoint ./checkpoint/wn18rr/checkpoint_epoch0.mdl
[2023-08-24 21:10:33,379 INFO] Epoch: [4][  0/169]	Loss 1.298 (1.298)	InvT  20.29 ( 20.29)	Acc@1  78.32 ( 78.32)	Acc@3  93.85 ( 93.85)
[2023-08-24 21:10:58,644 INFO] Epoch: [4][ 20/169]	Loss 1.139 (1.179)	InvT  20.31 ( 20.30)	Acc@1  81.25 ( 79.85)	Acc@3  94.82 ( 95.29)
[2023-08-24 21:11:24,251 INFO] Epoch: [4][ 40/169]	Loss 1.116 (1.156)	InvT  20.34 ( 20.31)	Acc@1  80.37 ( 80.10)	Acc@3  94.82 ( 95.41)
[2023-08-24 21:11:49,383 INFO] Epoch: [4][ 60/169]	Loss 1.215 (1.159)	InvT  20.36 ( 20.33)	Acc@1  78.61 ( 80.11)	Acc@3  94.63 ( 95.32)
[2023-08-24 21:12:14,339 INFO] Epoch: [4][ 80/169]	Loss 1.226 (1.157)	InvT  20.38 ( 20.34)	Acc@1  80.18 ( 80.25)	Acc@3  94.04 ( 95.26)
[2023-08-24 21:12:39,319 INFO] Epoch: [4][100/169]	Loss 1.173 (1.152)	InvT  20.40 ( 20.35)	Acc@1  79.88 ( 80.32)	Acc@3  94.43 ( 95.29)
[2023-08-24 21:13:04,379 INFO] Epoch: [4][120/169]	Loss 1.146 (1.15)	InvT  20.42 ( 20.36)	Acc@1  80.27 ( 80.34)	Acc@3  95.02 ( 95.30)
[2023-08-24 21:13:29,215 INFO] Epoch: [4][140/169]	Loss 1.205 (1.153)	InvT  20.44 ( 20.37)	Acc@1  79.98 ( 80.28)	Acc@3  94.92 ( 95.25)
[2023-08-24 21:13:54,104 INFO] Epoch: [4][160/169]	Loss 1.145 (1.152)	InvT  20.46 ( 20.38)	Acc@1  79.30 ( 80.28)	Acc@3  95.12 ( 95.24)
[2023-08-24 21:14:03,509 INFO] Learning rate: 4.7245946280480256e-05
[2023-08-24 21:14:11,299 INFO] Epoch 4, valid metric: {"Acc@1": 72.825, "Acc@3": 88.794, "loss": 1.026}
[2023-08-24 21:14:17,309 INFO] Delete old checkpoint ./checkpoint/wn18rr/checkpoint_epoch1.mdl
[2023-08-24 21:14:21,506 INFO] Epoch: [5][  0/169]	Loss 0.89 (0.89)	InvT  20.47 ( 20.47)	Acc@1  84.47 ( 84.47)	Acc@3  97.36 ( 97.36)
[2023-08-24 21:14:48,172 INFO] Epoch: [5][ 20/169]	Loss 0.9045 (0.8911)	InvT  20.49 ( 20.48)	Acc@1  84.57 ( 84.46)	Acc@3  97.27 ( 96.98)
[2023-08-24 21:15:13,135 INFO] Epoch: [5][ 40/169]	Loss 0.9175 (0.8837)	InvT  20.52 ( 20.49)	Acc@1  84.38 ( 84.62)	Acc@3  96.58 ( 96.94)
[2023-08-24 21:15:38,153 INFO] Epoch: [5][ 60/169]	Loss 0.8881 (0.8785)	InvT  20.54 ( 20.50)	Acc@1  85.25 ( 84.63)	Acc@3  96.97 ( 97.01)
[2023-08-24 21:16:02,852 INFO] Epoch: [5][ 80/169]	Loss 0.8933 (0.8784)	InvT  20.56 ( 20.52)	Acc@1  86.13 ( 84.73)	Acc@3  97.17 ( 97.01)
[2023-08-24 21:16:28,267 INFO] Epoch: [5][100/169]	Loss 0.8747 (0.8821)	InvT  20.58 ( 20.53)	Acc@1  84.18 ( 84.61)	Acc@3  97.56 ( 96.98)
[2023-08-24 21:16:53,283 INFO] Epoch: [5][120/169]	Loss 0.9221 (0.8844)	InvT  20.60 ( 20.54)	Acc@1  84.28 ( 84.62)	Acc@3  96.68 ( 96.99)
[2023-08-24 21:17:18,216 INFO] Epoch: [5][140/169]	Loss 0.912 (0.887)	InvT  20.62 ( 20.55)	Acc@1  85.16 ( 84.58)	Acc@3  96.88 ( 96.97)
[2023-08-24 21:17:43,479 INFO] Epoch: [5][160/169]	Loss 0.9685 (0.8893)	InvT  20.64 ( 20.56)	Acc@1  84.08 ( 84.54)	Acc@3  96.48 ( 96.94)
[2023-08-24 21:17:52,605 INFO] Learning rate: 4.6200024755539054e-05
[2023-08-24 21:18:00,508 INFO] Epoch 5, valid metric: {"Acc@1": 74.044, "Acc@3": 89.206, "loss": 1.02}
[2023-08-24 21:18:06,551 INFO] Delete old checkpoint ./checkpoint/wn18rr/checkpoint_epoch2.mdl
[2023-08-24 21:18:10,911 INFO] Epoch: [6][  0/169]	Loss 0.7042 (0.7042)	InvT  20.65 ( 20.65)	Acc@1  88.28 ( 88.28)	Acc@3  98.05 ( 98.05)
[2023-08-24 21:18:36,915 INFO] Epoch: [6][ 20/169]	Loss 0.7288 (0.6852)	InvT  20.67 ( 20.66)	Acc@1  87.99 ( 88.20)	Acc@3  97.95 ( 98.19)
[2023-08-24 21:19:03,743 INFO] Epoch: [6][ 40/169]	Loss 0.6863 (0.6911)	InvT  20.69 ( 20.67)	Acc@1  87.40 ( 87.96)	Acc@3  97.95 ( 98.19)
[2023-08-24 21:19:29,070 INFO] Epoch: [6][ 60/169]	Loss 0.7645 (0.6929)	InvT  20.71 ( 20.68)	Acc@1  87.70 ( 87.85)	Acc@3  97.46 ( 98.14)
[2023-08-24 21:19:55,708 INFO] Epoch: [6][ 80/169]	Loss 0.7487 (0.695)	InvT  20.73 ( 20.69)	Acc@1  87.40 ( 87.84)	Acc@3  98.34 ( 98.12)
[2023-08-24 21:20:21,105 INFO] Epoch: [6][100/169]	Loss 0.617 (0.6977)	InvT  20.75 ( 20.70)	Acc@1  89.36 ( 87.82)	Acc@3  98.44 ( 98.09)
[2023-08-24 21:20:46,090 INFO] Epoch: [6][120/169]	Loss 0.738 (0.7015)	InvT  20.77 ( 20.71)	Acc@1  88.38 ( 87.77)	Acc@3  97.75 ( 98.07)
[2023-08-24 21:21:11,169 INFO] Epoch: [6][140/169]	Loss 0.7001 (0.7044)	InvT  20.79 ( 20.72)	Acc@1  88.28 ( 87.71)	Acc@3  98.24 ( 98.05)
[2023-08-24 21:21:39,292 INFO] Epoch: [6][160/169]	Loss 0.7566 (0.7059)	InvT  20.80 ( 20.73)	Acc@1  86.82 ( 87.66)	Acc@3  97.85 ( 98.03)
[2023-08-24 21:21:49,346 INFO] Learning rate: 4.515410323059785e-05
[2023-08-24 21:21:58,796 INFO] Epoch 6, valid metric: {"Acc@1": 74.44, "Acc@3": 88.596, "loss": 1.009}
[2023-08-24 21:22:04,802 INFO] Delete old checkpoint ./checkpoint/wn18rr/checkpoint_epoch3.mdl
[2023-08-24 21:22:09,056 INFO] Epoch: [7][  0/169]	Loss 0.6179 (0.6179)	InvT  20.81 ( 20.81)	Acc@1  88.77 ( 88.77)	Acc@3  98.93 ( 98.93)
[2023-08-24 21:22:35,820 INFO] Epoch: [7][ 20/169]	Loss 0.5126 (0.5486)	InvT  20.83 ( 20.82)	Acc@1  91.60 ( 90.33)	Acc@3  99.02 ( 98.87)
[2023-08-24 21:23:01,127 INFO] Epoch: [7][ 40/169]	Loss 0.5076 (0.5471)	InvT  20.85 ( 20.83)	Acc@1  91.50 ( 90.31)	Acc@3  99.12 ( 98.87)
[2023-08-24 21:23:26,364 INFO] Epoch: [7][ 60/169]	Loss 0.5964 (0.5513)	InvT  20.87 ( 20.84)	Acc@1  90.82 ( 90.29)	Acc@3  98.73 ( 98.85)
[2023-08-24 21:23:51,612 INFO] Epoch: [7][ 80/169]	Loss 0.5485 (0.5565)	InvT  20.89 ( 20.85)	Acc@1  90.23 ( 90.23)	Acc@3  98.44 ( 98.83)
[2023-08-24 21:24:16,363 INFO] Epoch: [7][100/169]	Loss 0.5958 (0.5611)	InvT  20.91 ( 20.86)	Acc@1  89.75 ( 90.21)	Acc@3  98.24 ( 98.77)
[2023-08-24 21:24:41,457 INFO] Epoch: [7][120/169]	Loss 0.6485 (0.5646)	InvT  20.93 ( 20.87)	Acc@1  87.40 ( 90.12)	Acc@3  98.54 ( 98.74)
[2023-08-24 21:25:06,840 INFO] Epoch: [7][140/169]	Loss 0.563 (0.569)	InvT  20.94 ( 20.88)	Acc@1  90.92 ( 90.03)	Acc@3  98.34 ( 98.71)
[2023-08-24 21:25:31,736 INFO] Epoch: [7][160/169]	Loss 0.5961 (0.571)	InvT  20.96 ( 20.89)	Acc@1  90.43 ( 90.03)	Acc@3  98.73 ( 98.70)
[2023-08-24 21:25:40,743 INFO] Learning rate: 4.4108181705656643e-05
[2023-08-24 21:25:48,514 INFO] Epoch 7, valid metric: {"Acc@1": 74.374, "Acc@3": 88.909, "loss": 1.013}
[2023-08-24 21:25:52,986 INFO] Delete old checkpoint ./checkpoint/wn18rr/checkpoint_epoch4.mdl
[2023-08-24 21:25:57,229 INFO] Epoch: [8][  0/169]	Loss 0.5243 (0.5243)	InvT  20.97 ( 20.97)	Acc@1  90.72 ( 90.72)	Acc@3  98.54 ( 98.54)
[2023-08-24 21:26:23,684 INFO] Epoch: [8][ 20/169]	Loss 0.4996 (0.4718)	InvT  20.99 ( 20.98)	Acc@1  91.50 ( 91.93)	Acc@3  99.41 ( 99.21)
[2023-08-24 21:26:48,711 INFO] Epoch: [8][ 40/169]	Loss 0.4611 (0.4637)	InvT  21.00 ( 20.99)	Acc@1  92.97 ( 92.10)	Acc@3  98.93 ( 99.26)
[2023-08-24 21:27:13,687 INFO] Epoch: [8][ 60/169]	Loss 0.4526 (0.4673)	InvT  21.02 ( 21.00)	Acc@1  93.46 ( 92.00)	Acc@3  99.22 ( 99.23)
[2023-08-24 21:27:38,485 INFO] Epoch: [8][ 80/169]	Loss 0.4352 (0.4676)	InvT  21.04 ( 21.00)	Acc@1  93.55 ( 92.01)	Acc@3  99.41 ( 99.23)
[2023-08-24 21:28:03,272 INFO] Epoch: [8][100/169]	Loss 0.5915 (0.4691)	InvT  21.06 ( 21.01)	Acc@1  89.65 ( 91.94)	Acc@3  99.02 ( 99.22)
[2023-08-24 21:28:28,211 INFO] Epoch: [8][120/169]	Loss 0.4559 (0.4722)	InvT  21.07 ( 21.02)	Acc@1  92.87 ( 91.89)	Acc@3  99.32 ( 99.20)
[2023-08-24 21:28:52,935 INFO] Epoch: [8][140/169]	Loss 0.4501 (0.4719)	InvT  21.09 ( 21.03)	Acc@1  92.68 ( 91.91)	Acc@3  98.83 ( 99.18)
[2023-08-24 21:29:18,252 INFO] Epoch: [8][160/169]	Loss 0.4686 (0.4745)	InvT  21.11 ( 21.04)	Acc@1  92.48 ( 91.89)	Acc@3  99.12 ( 99.17)
[2023-08-24 21:29:27,388 INFO] Learning rate: 4.3062260180715435e-05
[2023-08-24 21:29:35,204 INFO] Epoch 8, valid metric: {"Acc@1": 74.506, "Acc@3": 88.678, "loss": 1.025}
[2023-08-24 21:29:41,163 INFO] Delete old checkpoint ./checkpoint/wn18rr/checkpoint_epoch5.mdl
[2023-08-24 21:29:45,379 INFO] Epoch: [9][  0/169]	Loss 0.3764 (0.3764)	InvT  21.12 ( 21.12)	Acc@1  93.26 ( 93.26)	Acc@3  99.32 ( 99.32)
[2023-08-24 21:30:11,538 INFO] Epoch: [9][ 20/169]	Loss 0.3744 (0.3846)	InvT  21.13 ( 21.12)	Acc@1  93.46 ( 93.55)	Acc@3  99.80 ( 99.60)
[2023-08-24 21:30:36,978 INFO] Epoch: [9][ 40/169]	Loss 0.3984 (0.383)	InvT  21.15 ( 21.13)	Acc@1  94.14 ( 93.64)	Acc@3  99.51 ( 99.60)
[2023-08-24 21:31:01,611 INFO] Epoch: [9][ 60/169]	Loss 0.3801 (0.3804)	InvT  21.17 ( 21.14)	Acc@1  92.58 ( 93.63)	Acc@3  99.71 ( 99.61)
[2023-08-24 21:31:26,606 INFO] Epoch: [9][ 80/169]	Loss 0.4 (0.383)	InvT  21.18 ( 21.15)	Acc@1  93.36 ( 93.59)	Acc@3  99.41 ( 99.59)
[2023-08-24 21:31:51,562 INFO] Epoch: [9][100/169]	Loss 0.3707 (0.3849)	InvT  21.20 ( 21.16)	Acc@1  93.46 ( 93.56)	Acc@3  99.71 ( 99.56)
[2023-08-24 21:32:16,467 INFO] Epoch: [9][120/169]	Loss 0.4101 (0.386)	InvT  21.21 ( 21.17)	Acc@1  93.16 ( 93.55)	Acc@3  99.61 ( 99.53)
[2023-08-24 21:32:41,326 INFO] Epoch: [9][140/169]	Loss 0.4039 (0.387)	InvT  21.23 ( 21.17)	Acc@1  93.85 ( 93.52)	Acc@3  99.51 ( 99.52)
[2023-08-24 21:33:06,356 INFO] Epoch: [9][160/169]	Loss 0.463 (0.39)	InvT  21.25 ( 21.18)	Acc@1  91.80 ( 93.46)	Acc@3  99.12 ( 99.50)
[2023-08-24 21:33:15,443 INFO] Learning rate: 4.2016338655774226e-05
[2023-08-24 21:33:23,814 INFO] Epoch 9, valid metric: {"Acc@1": 75.412, "Acc@3": 88.86, "loss": 1.025}
[2023-08-24 21:33:29,805 INFO] Delete old checkpoint ./checkpoint/wn18rr/checkpoint_epoch6.mdl
[2023-08-24 21:33:34,037 INFO] Epoch: [10][  0/169]	Loss 0.2843 (0.2843)	InvT  21.25 ( 21.25)	Acc@1  95.70 ( 95.70)	Acc@3  99.90 ( 99.90)
[2023-08-24 21:33:59,863 INFO] Epoch: [10][ 20/169]	Loss 0.3062 (0.3184)	InvT  21.27 ( 21.26)	Acc@1  95.31 ( 94.90)	Acc@3  99.71 ( 99.75)
[2023-08-24 21:34:24,432 INFO] Epoch: [10][ 40/169]	Loss 0.2924 (0.3188)	InvT  21.28 ( 21.27)	Acc@1  95.12 ( 94.86)	Acc@3  99.90 ( 99.75)
[2023-08-24 21:34:49,588 INFO] Epoch: [10][ 60/169]	Loss 0.3005 (0.317)	InvT  21.30 ( 21.28)	Acc@1  95.41 ( 94.87)	Acc@3  99.80 ( 99.73)
[2023-08-24 21:35:14,443 INFO] Epoch: [10][ 80/169]	Loss 0.3515 (0.3195)	InvT  21.31 ( 21.28)	Acc@1  93.55 ( 94.84)	Acc@3  99.61 ( 99.72)
[2023-08-24 21:35:39,440 INFO] Epoch: [10][100/169]	Loss 0.3666 (0.3229)	InvT  21.33 ( 21.29)	Acc@1  94.14 ( 94.74)	Acc@3  99.41 ( 99.70)
[2023-08-24 21:36:04,441 INFO] Epoch: [10][120/169]	Loss 0.3672 (0.3258)	InvT  21.35 ( 21.30)	Acc@1  92.77 ( 94.72)	Acc@3  99.32 ( 99.69)
[2023-08-24 21:36:29,499 INFO] Epoch: [10][140/169]	Loss 0.4017 (0.3312)	InvT  21.36 ( 21.31)	Acc@1  93.85 ( 94.61)	Acc@3  99.41 ( 99.67)
[2023-08-24 21:36:54,483 INFO] Epoch: [10][160/169]	Loss 0.3047 (0.3326)	InvT  21.38 ( 21.31)	Acc@1  95.90 ( 94.58)	Acc@3  99.80 ( 99.66)
[2023-08-24 21:37:03,641 INFO] Learning rate: 4.097041713083303e-05
[2023-08-24 21:37:11,442 INFO] Epoch 10, valid metric: {"Acc@1": 75.412, "Acc@3": 88.53, "loss": 1.056}
[2023-08-24 21:37:15,914 INFO] Delete old checkpoint ./checkpoint/wn18rr/checkpoint_epoch7.mdl
[2023-08-24 21:37:21,331 INFO] Epoch: [11][  0/169]	Loss 0.2675 (0.2675)	InvT  21.38 ( 21.38)	Acc@1  96.29 ( 96.29)	Acc@3  99.90 ( 99.90)
[2023-08-24 21:37:46,758 INFO] Epoch: [11][ 20/169]	Loss 0.2768 (0.2841)	InvT  21.40 ( 21.39)	Acc@1  95.70 ( 95.51)	Acc@3  99.71 ( 99.78)
[2023-08-24 21:38:11,663 INFO] Epoch: [11][ 40/169]	Loss 0.2246 (0.2792)	InvT  21.41 ( 21.40)	Acc@1  96.68 ( 95.57)	Acc@3 100.00 ( 99.83)
[2023-08-24 21:38:36,668 INFO] Epoch: [11][ 60/169]	Loss 0.2651 (0.2789)	InvT  21.43 ( 21.40)	Acc@1  95.51 ( 95.59)	Acc@3  99.80 ( 99.82)
[2023-08-24 21:39:01,960 INFO] Epoch: [11][ 80/169]	Loss 0.2809 (0.2799)	InvT  21.44 ( 21.41)	Acc@1  96.09 ( 95.61)	Acc@3  99.90 ( 99.79)
[2023-08-24 21:39:26,836 INFO] Epoch: [11][100/169]	Loss 0.2943 (0.2818)	InvT  21.45 ( 21.42)	Acc@1  95.51 ( 95.53)	Acc@3  99.90 ( 99.79)
[2023-08-24 21:39:51,728 INFO] Epoch: [11][120/169]	Loss 0.2988 (0.2838)	InvT  21.47 ( 21.43)	Acc@1  95.21 ( 95.50)	Acc@3  99.71 ( 99.78)
[2023-08-24 21:40:16,839 INFO] Epoch: [11][140/169]	Loss 0.3174 (0.2863)	InvT  21.48 ( 21.43)	Acc@1  94.53 ( 95.46)	Acc@3  99.71 ( 99.77)
[2023-08-24 21:40:41,525 INFO] Epoch: [11][160/169]	Loss 0.3106 (0.2862)	InvT  21.50 ( 21.44)	Acc@1  94.82 ( 95.46)	Acc@3  99.90 ( 99.77)
[2023-08-24 21:40:50,803 INFO] Learning rate: 3.992449560589182e-05
[2023-08-24 21:40:58,578 INFO] Epoch 11, valid metric: {"Acc@1": 75.346, "Acc@3": 88.233, "loss": 1.068}
[2023-08-24 21:41:03,014 INFO] Delete old checkpoint ./checkpoint/wn18rr/checkpoint_epoch8.mdl
[2023-08-24 21:41:07,127 INFO] Epoch: [12][  0/169]	Loss 0.2342 (0.2342)	InvT  21.50 ( 21.50)	Acc@1  96.88 ( 96.88)	Acc@3  99.80 ( 99.80)
[2023-08-24 21:41:32,980 INFO] Epoch: [12][ 20/169]	Loss 0.2441 (0.2404)	InvT  21.52 ( 21.51)	Acc@1  96.58 ( 96.44)	Acc@3 100.00 ( 99.92)
[2023-08-24 21:41:58,300 INFO] Epoch: [12][ 40/169]	Loss 0.2721 (0.2451)	InvT  21.53 ( 21.52)	Acc@1  96.00 ( 96.28)	Acc@3  99.80 ( 99.89)
[2023-08-24 21:42:23,018 INFO] Epoch: [12][ 60/169]	Loss 0.2459 (0.2456)	InvT  21.54 ( 21.52)	Acc@1  95.90 ( 96.26)	Acc@3  99.90 ( 99.86)
[2023-08-24 21:42:48,094 INFO] Epoch: [12][ 80/169]	Loss 0.2556 (0.2475)	InvT  21.56 ( 21.53)	Acc@1  96.78 ( 96.24)	Acc@3  99.71 ( 99.86)
[2023-08-24 21:43:13,139 INFO] Epoch: [12][100/169]	Loss 0.2545 (0.249)	InvT  21.57 ( 21.54)	Acc@1  96.00 ( 96.21)	Acc@3  99.80 ( 99.86)
[2023-08-24 21:43:38,106 INFO] Epoch: [12][120/169]	Loss 0.2738 (0.25)	InvT  21.59 ( 21.54)	Acc@1  96.48 ( 96.19)	Acc@3  99.71 ( 99.85)
[2023-08-24 21:44:02,888 INFO] Epoch: [12][140/169]	Loss 0.2722 (0.2511)	InvT  21.60 ( 21.55)	Acc@1  95.80 ( 96.17)	Acc@3  99.80 ( 99.84)
[2023-08-24 21:44:27,903 INFO] Epoch: [12][160/169]	Loss 0.2867 (0.2531)	InvT  21.61 ( 21.56)	Acc@1  95.61 ( 96.15)	Acc@3 100.00 ( 99.84)
[2023-08-24 21:44:37,032 INFO] Learning rate: 3.887857408095061e-05
[2023-08-24 21:44:45,370 INFO] Epoch 12, valid metric: {"Acc@1": 75.61, "Acc@3": 88.316, "loss": 1.054}
[2023-08-24 21:44:51,311 INFO] Delete old checkpoint ./checkpoint/wn18rr/checkpoint_epoch9.mdl
[2023-08-24 21:44:55,471 INFO] Epoch: [13][  0/169]	Loss 0.1943 (0.1943)	InvT  21.62 ( 21.62)	Acc@1  97.17 ( 97.17)	Acc@3 100.00 (100.00)
[2023-08-24 21:45:21,574 INFO] Epoch: [13][ 20/169]	Loss 0.2206 (0.2155)	InvT  21.63 ( 21.63)	Acc@1  97.17 ( 96.83)	Acc@3 100.00 ( 99.91)
[2023-08-24 21:45:46,148 INFO] Epoch: [13][ 40/169]	Loss 0.2481 (0.216)	InvT  21.64 ( 21.63)	Acc@1  96.19 ( 96.80)	Acc@3  99.61 ( 99.90)
[2023-08-24 21:46:11,268 INFO] Epoch: [13][ 60/169]	Loss 0.2219 (0.2147)	InvT  21.66 ( 21.64)	Acc@1  95.90 ( 96.78)	Acc@3  99.80 ( 99.90)
[2023-08-24 21:46:36,254 INFO] Epoch: [13][ 80/169]	Loss 0.2774 (0.216)	InvT  21.67 ( 21.64)	Acc@1  96.00 ( 96.76)	Acc@3 100.00 ( 99.90)
[2023-08-24 21:47:00,894 INFO] Epoch: [13][100/169]	Loss 0.2306 (0.2189)	InvT  21.68 ( 21.65)	Acc@1  96.58 ( 96.71)	Acc@3  99.90 ( 99.89)
[2023-08-24 21:47:25,848 INFO] Epoch: [13][120/169]	Loss 0.2125 (0.2208)	InvT  21.69 ( 21.66)	Acc@1  96.88 ( 96.69)	Acc@3  99.80 ( 99.90)
[2023-08-24 21:47:50,903 INFO] Epoch: [13][140/169]	Loss 0.1963 (0.2207)	InvT  21.71 ( 21.66)	Acc@1  97.07 ( 96.69)	Acc@3  99.61 ( 99.90)
[2023-08-24 21:48:15,960 INFO] Epoch: [13][160/169]	Loss 0.2011 (0.221)	InvT  21.72 ( 21.67)	Acc@1  97.85 ( 96.68)	Acc@3 100.00 ( 99.89)
[2023-08-24 21:48:25,038 INFO] Learning rate: 3.7832652556009404e-05
[2023-08-24 21:48:32,817 INFO] Epoch 13, valid metric: {"Acc@1": 75.461, "Acc@3": 88.233, "loss": 1.077}
[2023-08-24 21:48:37,310 INFO] Delete old checkpoint ./checkpoint/wn18rr/checkpoint_epoch10.mdl
[2023-08-24 21:48:41,498 INFO] Epoch: [14][  0/169]	Loss 0.1788 (0.1788)	InvT  21.73 ( 21.73)	Acc@1  97.36 ( 97.36)	Acc@3 100.00 (100.00)
[2023-08-24 21:49:08,102 INFO] Epoch: [14][ 20/169]	Loss 0.1968 (0.1816)	InvT  21.74 ( 21.73)	Acc@1  96.88 ( 97.58)	Acc@3 100.00 ( 99.93)
[2023-08-24 21:49:33,019 INFO] Epoch: [14][ 40/169]	Loss 0.1893 (0.1842)	InvT  21.75 ( 21.74)	Acc@1  97.17 ( 97.42)	Acc@3  99.90 ( 99.94)
[2023-08-24 21:49:58,088 INFO] Epoch: [14][ 60/169]	Loss 0.2073 (0.1869)	InvT  21.76 ( 21.75)	Acc@1  96.88 ( 97.38)	Acc@3  99.90 ( 99.94)
[2023-08-24 21:50:22,860 INFO] Epoch: [14][ 80/169]	Loss 0.1931 (0.1885)	InvT  21.78 ( 21.75)	Acc@1  97.56 ( 97.34)	Acc@3  99.80 ( 99.95)
[2023-08-24 21:50:48,112 INFO] Epoch: [14][100/169]	Loss 0.2069 (0.1894)	InvT  21.79 ( 21.76)	Acc@1  97.17 ( 97.34)	Acc@3 100.00 ( 99.94)
[2023-08-24 21:51:13,157 INFO] Epoch: [14][120/169]	Loss 0.1867 (0.1909)	InvT  21.80 ( 21.76)	Acc@1  97.85 ( 97.31)	Acc@3 100.00 ( 99.93)
[2023-08-24 21:51:37,729 INFO] Epoch: [14][140/169]	Loss 0.2066 (0.1922)	InvT  21.81 ( 21.77)	Acc@1  96.48 ( 97.29)	Acc@3 100.00 ( 99.93)
[2023-08-24 21:52:02,719 INFO] Epoch: [14][160/169]	Loss 0.2312 (0.1933)	InvT  21.82 ( 21.78)	Acc@1  96.68 ( 97.24)	Acc@3 100.00 ( 99.93)
[2023-08-24 21:52:11,920 INFO] Learning rate: 3.67867310310682e-05
[2023-08-24 21:52:19,705 INFO] Epoch 14, valid metric: {"Acc@1": 76.005, "Acc@3": 88.316, "loss": 1.061}
[2023-08-24 21:52:25,675 INFO] Delete old checkpoint ./checkpoint/wn18rr/checkpoint_epoch11.mdl
[2023-08-24 21:52:29,850 INFO] Epoch: [15][  0/169]	Loss 0.1378 (0.1378)	InvT  21.83 ( 21.83)	Acc@1  98.34 ( 98.34)	Acc@3  99.90 ( 99.90)
[2023-08-24 21:52:55,978 INFO] Epoch: [15][ 20/169]	Loss 0.1578 (0.1689)	InvT  21.84 ( 21.84)	Acc@1  97.56 ( 97.60)	Acc@3 100.00 ( 99.93)
[2023-08-24 21:53:20,622 INFO] Epoch: [15][ 40/169]	Loss 0.1859 (0.1696)	InvT  21.85 ( 21.84)	Acc@1  97.85 ( 97.64)	Acc@3  99.90 ( 99.94)
[2023-08-24 21:53:45,884 INFO] Epoch: [15][ 60/169]	Loss 0.1443 (0.1691)	InvT  21.86 ( 21.85)	Acc@1  98.24 ( 97.71)	Acc@3  99.80 ( 99.94)
[2023-08-24 21:54:11,003 INFO] Epoch: [15][ 80/169]	Loss 0.1805 (0.1704)	InvT  21.88 ( 21.85)	Acc@1  97.36 ( 97.68)	Acc@3 100.00 ( 99.95)
[2023-08-24 21:54:35,607 INFO] Epoch: [15][100/169]	Loss 0.1616 (0.1714)	InvT  21.89 ( 21.86)	Acc@1  98.44 ( 97.68)	Acc@3 100.00 ( 99.96)
[2023-08-24 21:55:00,719 INFO] Epoch: [15][120/169]	Loss 0.1642 (0.1721)	InvT  21.90 ( 21.86)	Acc@1  97.75 ( 97.65)	Acc@3  99.90 ( 99.95)
[2023-08-24 21:55:25,887 INFO] Epoch: [15][140/169]	Loss 0.182 (0.1727)	InvT  21.91 ( 21.87)	Acc@1  98.14 ( 97.63)	Acc@3  99.90 ( 99.95)
[2023-08-24 21:55:50,810 INFO] Epoch: [15][160/169]	Loss 0.2114 (0.1738)	InvT  21.92 ( 21.88)	Acc@1  96.78 ( 97.59)	Acc@3  99.80 ( 99.95)
[2023-08-24 21:55:59,891 INFO] Learning rate: 3.5740809506127e-05
[2023-08-24 21:56:07,702 INFO] Epoch 15, valid metric: {"Acc@1": 75.758, "Acc@3": 87.805, "loss": 1.081}
[2023-08-24 21:56:12,132 INFO] Delete old checkpoint ./checkpoint/wn18rr/checkpoint_epoch12.mdl
[2023-08-24 21:56:16,331 INFO] Epoch: [16][  0/169]	Loss 0.1676 (0.1676)	InvT  21.93 ( 21.93)	Acc@1  97.56 ( 97.56)	Acc@3 100.00 (100.00)
[2023-08-24 21:56:42,007 INFO] Epoch: [16][ 20/169]	Loss 0.1635 (0.1514)	InvT  21.94 ( 21.93)	Acc@1  98.14 ( 98.08)	Acc@3 100.00 ( 99.97)
[2023-08-24 21:57:06,893 INFO] Epoch: [16][ 40/169]	Loss 0.158 (0.1522)	InvT  21.95 ( 21.94)	Acc@1  98.14 ( 98.04)	Acc@3 100.00 ( 99.98)
[2023-08-24 21:57:31,944 INFO] Epoch: [16][ 60/169]	Loss 0.1717 (0.1524)	InvT  21.96 ( 21.94)	Acc@1  98.14 ( 98.05)	Acc@3 100.00 ( 99.97)
[2023-08-24 21:57:57,215 INFO] Epoch: [16][ 80/169]	Loss 0.1414 (0.1525)	InvT  21.97 ( 21.95)	Acc@1  97.85 ( 98.02)	Acc@3 100.00 ( 99.97)
[2023-08-24 21:58:22,172 INFO] Epoch: [16][100/169]	Loss 0.1532 (0.1535)	InvT  21.98 ( 21.96)	Acc@1  98.14 ( 97.99)	Acc@3 100.00 ( 99.97)
[2023-08-24 21:58:47,097 INFO] Epoch: [16][120/169]	Loss 0.1505 (0.1547)	InvT  21.99 ( 21.96)	Acc@1  97.56 ( 97.96)	Acc@3 100.00 ( 99.97)
[2023-08-24 21:59:12,152 INFO] Epoch: [16][140/169]	Loss 0.1868 (0.1552)	InvT  22.01 ( 21.97)	Acc@1  97.17 ( 97.95)	Acc@3 100.00 ( 99.97)
[2023-08-24 21:59:37,111 INFO] Epoch: [16][160/169]	Loss 0.2113 (0.1566)	InvT  22.02 ( 21.97)	Acc@1  96.39 ( 97.90)	Acc@3 100.00 ( 99.96)
[2023-08-24 21:59:46,319 INFO] Learning rate: 3.469488798118579e-05
[2023-08-24 21:59:54,102 INFO] Epoch 16, valid metric: {"Acc@1": 76.038, "Acc@3": 88.349, "loss": 1.081}
[2023-08-24 22:00:00,102 INFO] Delete old checkpoint ./checkpoint/wn18rr/checkpoint_epoch13.mdl
[2023-08-24 22:00:04,362 INFO] Epoch: [17][  0/169]	Loss 0.1273 (0.1273)	InvT  22.02 ( 22.02)	Acc@1  98.93 ( 98.93)	Acc@3 100.00 (100.00)
[2023-08-24 22:00:30,366 INFO] Epoch: [17][ 20/169]	Loss 0.14 (0.1377)	InvT  22.03 ( 22.03)	Acc@1  98.44 ( 98.36)	Acc@3 100.00 ( 99.98)
[2023-08-24 22:00:55,252 INFO] Epoch: [17][ 40/169]	Loss 0.1424 (0.1379)	InvT  22.04 ( 22.03)	Acc@1  98.83 ( 98.37)	Acc@3 100.00 ( 99.97)
[2023-08-24 22:01:20,138 INFO] Epoch: [17][ 60/169]	Loss 0.1271 (0.1407)	InvT  22.05 ( 22.04)	Acc@1  98.44 ( 98.28)	Acc@3 100.00 ( 99.98)
[2023-08-24 22:01:44,993 INFO] Epoch: [17][ 80/169]	Loss 0.1451 (0.142)	InvT  22.06 ( 22.04)	Acc@1  97.75 ( 98.21)	Acc@3 100.00 ( 99.98)
[2023-08-24 22:02:10,043 INFO] Epoch: [17][100/169]	Loss 0.1479 (0.143)	InvT  22.07 ( 22.05)	Acc@1  98.54 ( 98.18)	Acc@3  99.90 ( 99.98)
[2023-08-24 22:02:34,932 INFO] Epoch: [17][120/169]	Loss 0.1614 (0.1434)	InvT  22.08 ( 22.05)	Acc@1  97.95 ( 98.16)	Acc@3 100.00 ( 99.98)
[2023-08-24 22:02:59,834 INFO] Epoch: [17][140/169]	Loss 0.1607 (0.1444)	InvT  22.10 ( 22.06)	Acc@1  97.75 ( 98.15)	Acc@3  99.80 ( 99.98)
[2023-08-24 22:03:24,894 INFO] Epoch: [17][160/169]	Loss 0.1295 (0.1451)	InvT  22.11 ( 22.06)	Acc@1  98.54 ( 98.13)	Acc@3 100.00 ( 99.98)
[2023-08-24 22:03:34,139 INFO] Learning rate: 3.364896645624458e-05
[2023-08-24 22:03:41,909 INFO] Epoch 17, valid metric: {"Acc@1": 75.939, "Acc@3": 87.937, "loss": 1.1}
[2023-08-24 22:03:46,380 INFO] Delete old checkpoint ./checkpoint/wn18rr/checkpoint_epoch14.mdl
[2023-08-24 22:03:50,570 INFO] Epoch: [18][  0/169]	Loss 0.1012 (0.1012)	InvT  22.11 ( 22.11)	Acc@1  98.54 ( 98.54)	Acc@3 100.00 (100.00)
[2023-08-24 22:04:16,400 INFO] Epoch: [18][ 20/169]	Loss 0.1204 (0.124)	InvT  22.12 ( 22.12)	Acc@1  98.54 ( 98.47)	Acc@3 100.00 (100.00)
[2023-08-24 22:04:41,057 INFO] Epoch: [18][ 40/169]	Loss 0.1657 (0.1247)	InvT  22.13 ( 22.12)	Acc@1  97.17 ( 98.45)	Acc@3 100.00 (100.00)
[2023-08-24 22:05:06,097 INFO] Epoch: [18][ 60/169]	Loss 0.1101 (0.1254)	InvT  22.14 ( 22.13)	Acc@1  98.83 ( 98.41)	Acc@3 100.00 (100.00)
[2023-08-24 22:05:31,356 INFO] Epoch: [18][ 80/169]	Loss 0.1425 (0.1265)	InvT  22.15 ( 22.13)	Acc@1  97.36 ( 98.40)	Acc@3 100.00 ( 99.99)
[2023-08-24 22:05:55,912 INFO] Epoch: [18][100/169]	Loss 0.1489 (0.1276)	InvT  22.16 ( 22.14)	Acc@1  98.63 ( 98.38)	Acc@3  99.90 ( 99.98)
[2023-08-24 22:06:20,790 INFO] Epoch: [18][120/169]	Loss 0.1386 (0.1298)	InvT  22.17 ( 22.14)	Acc@1  98.63 ( 98.34)	Acc@3 100.00 ( 99.98)
[2023-08-24 22:06:45,732 INFO] Epoch: [18][140/169]	Loss 0.1138 (0.1302)	InvT  22.18 ( 22.15)	Acc@1  99.22 ( 98.31)	Acc@3 100.00 ( 99.98)
[2023-08-24 22:07:10,688 INFO] Epoch: [18][160/169]	Loss 0.1471 (0.1307)	InvT  22.19 ( 22.15)	Acc@1  98.05 ( 98.30)	Acc@3  99.90 ( 99.98)
[2023-08-24 22:07:19,925 INFO] Learning rate: 3.260304493130338e-05
[2023-08-24 22:07:27,691 INFO] Epoch 18, valid metric: {"Acc@1": 75.692, "Acc@3": 88.167, "loss": 1.093}
[2023-08-24 22:07:32,139 INFO] Delete old checkpoint ./checkpoint/wn18rr/checkpoint_epoch15.mdl
[2023-08-24 22:07:37,776 INFO] Epoch: [19][  0/169]	Loss 0.1007 (0.1007)	InvT  22.20 ( 22.20)	Acc@1  98.83 ( 98.83)	Acc@3 100.00 (100.00)
[2023-08-24 22:08:03,047 INFO] Epoch: [19][ 20/169]	Loss 0.115 (0.115)	InvT  22.21 ( 22.20)	Acc@1  98.73 ( 98.58)	Acc@3 100.00 ( 99.99)
[2023-08-24 22:08:28,028 INFO] Epoch: [19][ 40/169]	Loss 0.1257 (0.1153)	InvT  22.22 ( 22.21)	Acc@1  98.44 ( 98.55)	Acc@3 100.00 ( 99.99)
[2023-08-24 22:08:52,899 INFO] Epoch: [19][ 60/169]	Loss 0.1301 (0.1184)	InvT  22.23 ( 22.21)	Acc@1  97.75 ( 98.48)	Acc@3 100.00 ( 99.99)
[2023-08-24 22:09:17,846 INFO] Epoch: [19][ 80/169]	Loss 0.125 (0.1204)	InvT  22.24 ( 22.22)	Acc@1  98.24 ( 98.47)	Acc@3  99.90 ( 99.99)
[2023-08-24 22:09:42,820 INFO] Epoch: [19][100/169]	Loss 0.1262 (0.1207)	InvT  22.25 ( 22.22)	Acc@1  98.14 ( 98.48)	Acc@3 100.00 ( 99.99)
[2023-08-24 22:10:07,703 INFO] Epoch: [19][120/169]	Loss 0.1056 (0.1219)	InvT  22.26 ( 22.23)	Acc@1  98.93 ( 98.46)	Acc@3 100.00 ( 99.98)
[2023-08-24 22:10:32,219 INFO] Epoch: [19][140/169]	Loss 0.1212 (0.1219)	InvT  22.27 ( 22.23)	Acc@1  98.93 ( 98.46)	Acc@3 100.00 ( 99.98)
[2023-08-24 22:10:57,455 INFO] Epoch: [19][160/169]	Loss 0.1257 (0.1221)	InvT  22.28 ( 22.24)	Acc@1  98.14 ( 98.45)	Acc@3 100.00 ( 99.98)
[2023-08-24 22:11:06,624 INFO] Learning rate: 3.155712340636218e-05
[2023-08-24 22:11:14,431 INFO] Epoch 19, valid metric: {"Acc@1": 76.17, "Acc@3": 87.986, "loss": 1.102}
[2023-08-24 22:11:20,337 INFO] Delete old checkpoint ./checkpoint/wn18rr/checkpoint_epoch16.mdl
[2023-08-24 22:11:24,534 INFO] Epoch: [20][  0/169]	Loss 0.1184 (0.1184)	InvT  22.28 ( 22.28)	Acc@1  98.54 ( 98.54)	Acc@3 100.00 (100.00)
[2023-08-24 22:11:50,540 INFO] Epoch: [20][ 20/169]	Loss 0.1151 (0.1106)	InvT  22.29 ( 22.28)	Acc@1  98.83 ( 98.70)	Acc@3 100.00 (100.00)
[2023-08-24 22:12:15,599 INFO] Epoch: [20][ 40/169]	Loss 0.1254 (0.1112)	InvT  22.30 ( 22.29)	Acc@1  98.73 ( 98.64)	Acc@3 100.00 (100.00)
[2023-08-24 22:12:40,681 INFO] Epoch: [20][ 60/169]	Loss 0.09241 (0.1114)	InvT  22.31 ( 22.29)	Acc@1  99.22 ( 98.66)	Acc@3 100.00 (100.00)
[2023-08-24 22:13:05,716 INFO] Epoch: [20][ 80/169]	Loss 0.09708 (0.1124)	InvT  22.32 ( 22.30)	Acc@1  98.93 ( 98.64)	Acc@3 100.00 ( 99.99)
[2023-08-24 22:13:30,251 INFO] Epoch: [20][100/169]	Loss 0.129 (0.1126)	InvT  22.33 ( 22.30)	Acc@1  98.24 ( 98.61)	Acc@3 100.00 ( 99.99)
[2023-08-24 22:13:55,410 INFO] Epoch: [20][120/169]	Loss 0.1171 (0.1137)	InvT  22.34 ( 22.31)	Acc@1  98.34 ( 98.58)	Acc@3 100.00 ( 99.99)
[2023-08-24 22:14:20,468 INFO] Epoch: [20][140/169]	Loss 0.09048 (0.1134)	InvT  22.35 ( 22.31)	Acc@1  99.22 ( 98.57)	Acc@3 100.00 ( 99.99)
[2023-08-24 22:14:45,195 INFO] Epoch: [20][160/169]	Loss 0.1131 (0.1139)	InvT  22.36 ( 22.32)	Acc@1  98.93 ( 98.56)	Acc@3 100.00 ( 99.99)
[2023-08-24 22:14:54,394 INFO] Learning rate: 3.051120188142097e-05
[2023-08-24 22:15:02,150 INFO] Epoch 20, valid metric: {"Acc@1": 75.593, "Acc@3": 87.821, "loss": 1.121}
[2023-08-24 22:15:06,655 INFO] Delete old checkpoint ./checkpoint/wn18rr/checkpoint_epoch17.mdl
[2023-08-24 22:15:10,759 INFO] Epoch: [21][  0/169]	Loss 0.09738 (0.09738)	InvT  22.36 ( 22.36)	Acc@1  98.83 ( 98.83)	Acc@3 100.00 (100.00)
[2023-08-24 22:15:37,338 INFO] Epoch: [21][ 20/169]	Loss 0.123 (0.1031)	InvT  22.37 ( 22.36)	Acc@1  98.14 ( 98.77)	Acc@3 100.00 ( 99.98)
[2023-08-24 22:16:02,207 INFO] Epoch: [21][ 40/169]	Loss 0.08491 (0.1036)	InvT  22.38 ( 22.37)	Acc@1  99.02 ( 98.74)	Acc@3 100.00 ( 99.99)
[2023-08-24 22:16:26,972 INFO] Epoch: [21][ 60/169]	Loss 0.1128 (0.1045)	InvT  22.39 ( 22.37)	Acc@1  98.05 ( 98.72)	Acc@3 100.00 ( 99.99)
[2023-08-24 22:16:51,904 INFO] Epoch: [21][ 80/169]	Loss 0.09794 (0.1042)	InvT  22.40 ( 22.38)	Acc@1  98.54 ( 98.71)	Acc@3 100.00 ( 99.99)
[2023-08-24 22:17:17,128 INFO] Epoch: [21][100/169]	Loss 0.1162 (0.1049)	InvT  22.40 ( 22.38)	Acc@1  98.54 ( 98.68)	Acc@3 100.00 ( 99.99)
[2023-08-24 22:17:42,140 INFO] Epoch: [21][120/169]	Loss 0.1181 (0.1056)	InvT  22.41 ( 22.39)	Acc@1  98.34 ( 98.68)	Acc@3 100.00 ( 99.99)
[2023-08-24 22:18:06,871 INFO] Epoch: [21][140/169]	Loss 0.09601 (0.1055)	InvT  22.42 ( 22.39)	Acc@1  98.93 ( 98.67)	Acc@3 100.00 ( 99.99)
[2023-08-24 22:18:31,726 INFO] Epoch: [21][160/169]	Loss 0.1099 (0.1054)	InvT  22.43 ( 22.40)	Acc@1  98.63 ( 98.67)	Acc@3 100.00 ( 99.99)
[2023-08-24 22:18:40,970 INFO] Learning rate: 2.9465280356479765e-05
[2023-08-24 22:18:48,752 INFO] Epoch 21, valid metric: {"Acc@1": 75.527, "Acc@3": 87.508, "loss": 1.126}
[2023-08-24 22:18:53,197 INFO] Delete old checkpoint ./checkpoint/wn18rr/checkpoint_epoch18.mdl
[2023-08-24 22:18:57,446 INFO] Epoch: [22][  0/169]	Loss 0.07868 (0.07868)	InvT  22.44 ( 22.44)	Acc@1  99.51 ( 99.51)	Acc@3 100.00 (100.00)
[2023-08-24 22:19:23,632 INFO] Epoch: [22][ 20/169]	Loss 0.09375 (0.09231)	InvT  22.45 ( 22.44)	Acc@1  99.12 ( 98.82)	Acc@3 100.00 (100.00)
[2023-08-24 22:19:48,984 INFO] Epoch: [22][ 40/169]	Loss 0.117 (0.09337)	InvT  22.45 ( 22.45)	Acc@1  98.73 ( 98.81)	Acc@3 100.00 (100.00)
[2023-08-24 22:20:13,648 INFO] Epoch: [22][ 60/169]	Loss 0.09208 (0.09421)	InvT  22.46 ( 22.45)	Acc@1  98.73 ( 98.81)	Acc@3 100.00 (100.00)
[2023-08-24 22:20:38,550 INFO] Epoch: [22][ 80/169]	Loss 0.08854 (0.09582)	InvT  22.47 ( 22.45)	Acc@1  98.83 ( 98.80)	Acc@3 100.00 (100.00)
[2023-08-24 22:21:03,608 INFO] Epoch: [22][100/169]	Loss 0.09789 (0.09523)	InvT  22.48 ( 22.46)	Acc@1  98.83 ( 98.81)	Acc@3 100.00 (100.00)
[2023-08-24 22:21:28,411 INFO] Epoch: [22][120/169]	Loss 0.1088 (0.09545)	InvT  22.49 ( 22.46)	Acc@1  98.63 ( 98.82)	Acc@3 100.00 (100.00)
[2023-08-24 22:21:53,238 INFO] Epoch: [22][140/169]	Loss 0.114 (0.09644)	InvT  22.50 ( 22.47)	Acc@1  98.54 ( 98.80)	Acc@3  99.90 (100.00)
[2023-08-24 22:22:18,009 INFO] Epoch: [22][160/169]	Loss 0.09147 (0.09732)	InvT  22.51 ( 22.47)	Acc@1  98.93 ( 98.79)	Acc@3 100.00 (100.00)
[2023-08-24 22:22:27,144 INFO] Learning rate: 2.8419358831538556e-05
[2023-08-24 22:22:35,448 INFO] Epoch 22, valid metric: {"Acc@1": 76.417, "Acc@3": 87.673, "loss": 1.121}
[2023-08-24 22:22:41,368 INFO] Delete old checkpoint ./checkpoint/wn18rr/checkpoint_epoch19.mdl
[2023-08-24 22:22:45,610 INFO] Epoch: [23][  0/169]	Loss 0.08262 (0.08262)	InvT  22.51 ( 22.51)	Acc@1  98.83 ( 98.83)	Acc@3 100.00 (100.00)
[2023-08-24 22:23:11,658 INFO] Epoch: [23][ 20/169]	Loss 0.07775 (0.09321)	InvT  22.52 ( 22.51)	Acc@1  99.22 ( 98.90)	Acc@3 100.00 ( 99.99)
[2023-08-24 22:23:36,532 INFO] Epoch: [23][ 40/169]	Loss 0.09115 (0.09247)	InvT  22.53 ( 22.52)	Acc@1  98.93 ( 98.87)	Acc@3 100.00 ( 99.99)
[2023-08-24 22:24:01,775 INFO] Epoch: [23][ 60/169]	Loss 0.08541 (0.0918)	InvT  22.54 ( 22.52)	Acc@1  98.83 ( 98.90)	Acc@3 100.00 ( 99.99)
[2023-08-24 22:24:26,851 INFO] Epoch: [23][ 80/169]	Loss 0.08755 (0.09153)	InvT  22.54 ( 22.53)	Acc@1  99.32 ( 98.93)	Acc@3 100.00 ( 99.99)
[2023-08-24 22:24:51,888 INFO] Epoch: [23][100/169]	Loss 0.09701 (0.09172)	InvT  22.55 ( 22.53)	Acc@1  99.02 ( 98.92)	Acc@3 100.00 ( 99.99)
[2023-08-24 22:25:16,551 INFO] Epoch: [23][120/169]	Loss 0.09343 (0.09151)	InvT  22.56 ( 22.54)	Acc@1  99.02 ( 98.92)	Acc@3 100.00 ( 99.99)
[2023-08-24 22:25:41,963 INFO] Epoch: [23][140/169]	Loss 0.1042 (0.09113)	InvT  22.57 ( 22.54)	Acc@1  98.73 ( 98.94)	Acc@3 100.00 ( 99.99)
[2023-08-24 22:26:06,857 INFO] Epoch: [23][160/169]	Loss 0.09951 (0.09194)	InvT  22.58 ( 22.54)	Acc@1  98.73 ( 98.93)	Acc@3 100.00 ( 99.99)
[2023-08-24 22:26:15,995 INFO] Learning rate: 2.7373437306597354e-05
[2023-08-24 22:26:23,830 INFO] Epoch 23, valid metric: {"Acc@1": 76.615, "Acc@3": 87.508, "loss": 1.135}
[2023-08-24 22:26:29,866 INFO] Delete old checkpoint ./checkpoint/wn18rr/checkpoint_epoch20.mdl
[2023-08-24 22:26:34,204 INFO] Epoch: [24][  0/169]	Loss 0.08753 (0.08753)	InvT  22.58 ( 22.58)	Acc@1  98.93 ( 98.93)	Acc@3 100.00 (100.00)
[2023-08-24 22:27:00,699 INFO] Epoch: [24][ 20/169]	Loss 0.07702 (0.08361)	InvT  22.59 ( 22.59)	Acc@1  99.22 ( 98.98)	Acc@3 100.00 (100.00)
[2023-08-24 22:27:25,877 INFO] Epoch: [24][ 40/169]	Loss 0.08216 (0.08226)	InvT  22.60 ( 22.59)	Acc@1  99.02 ( 99.02)	Acc@3 100.00 (100.00)
[2023-08-24 22:27:50,919 INFO] Epoch: [24][ 60/169]	Loss 0.09036 (0.08355)	InvT  22.61 ( 22.59)	Acc@1  98.63 ( 98.98)	Acc@3 100.00 (100.00)
[2023-08-24 22:28:15,648 INFO] Epoch: [24][ 80/169]	Loss 0.07259 (0.08342)	InvT  22.61 ( 22.60)	Acc@1  99.32 ( 99.00)	Acc@3 100.00 (100.00)
[2023-08-24 22:28:40,984 INFO] Epoch: [24][100/169]	Loss 0.06882 (0.08398)	InvT  22.62 ( 22.60)	Acc@1  99.51 ( 98.98)	Acc@3 100.00 (100.00)
[2023-08-24 22:29:05,963 INFO] Epoch: [24][120/169]	Loss 0.1024 (0.08411)	InvT  22.63 ( 22.61)	Acc@1  98.63 ( 98.99)	Acc@3 100.00 (100.00)
[2023-08-24 22:29:30,991 INFO] Epoch: [24][140/169]	Loss 0.08447 (0.08447)	InvT  22.64 ( 22.61)	Acc@1  99.32 ( 98.98)	Acc@3 100.00 (100.00)
[2023-08-24 22:29:55,824 INFO] Epoch: [24][160/169]	Loss 0.07216 (0.0849)	InvT  22.65 ( 22.61)	Acc@1  99.02 ( 98.97)	Acc@3 100.00 (100.00)
[2023-08-24 22:30:04,910 INFO] Learning rate: 2.632751578165615e-05
[2023-08-24 22:30:12,714 INFO] Epoch 24, valid metric: {"Acc@1": 75.89, "Acc@3": 87.657, "loss": 1.135}
[2023-08-24 22:30:17,270 INFO] Delete old checkpoint ./checkpoint/wn18rr/checkpoint_epoch21.mdl
[2023-08-24 22:30:21,424 INFO] Epoch: [25][  0/169]	Loss 0.08458 (0.08458)	InvT  22.65 ( 22.65)	Acc@1  98.83 ( 98.83)	Acc@3 100.00 (100.00)
[2023-08-24 22:30:47,912 INFO] Epoch: [25][ 20/169]	Loss 0.09191 (0.0795)	InvT  22.66 ( 22.65)	Acc@1  99.12 ( 99.15)	Acc@3 100.00 ( 99.99)
[2023-08-24 22:31:12,876 INFO] Epoch: [25][ 40/169]	Loss 0.07957 (0.07799)	InvT  22.67 ( 22.66)	Acc@1  99.02 ( 99.14)	Acc@3 100.00 ( 99.99)
[2023-08-24 22:31:37,831 INFO] Epoch: [25][ 60/169]	Loss 0.07382 (0.07972)	InvT  22.67 ( 22.66)	Acc@1  99.32 ( 99.07)	Acc@3 100.00 ( 99.99)
[2023-08-24 22:32:02,612 INFO] Epoch: [25][ 80/169]	Loss 0.06984 (0.07932)	InvT  22.68 ( 22.67)	Acc@1  99.32 ( 99.09)	Acc@3 100.00 ( 99.99)
[2023-08-24 22:32:27,583 INFO] Epoch: [25][100/169]	Loss 0.07671 (0.0796)	InvT  22.69 ( 22.67)	Acc@1  99.32 ( 99.09)	Acc@3 100.00 ( 99.99)
[2023-08-24 22:32:52,931 INFO] Epoch: [25][120/169]	Loss 0.07052 (0.07928)	InvT  22.70 ( 22.67)	Acc@1  99.22 ( 99.09)	Acc@3 100.00 ( 99.99)
[2023-08-24 22:33:17,898 INFO] Epoch: [25][140/169]	Loss 0.07151 (0.07919)	InvT  22.71 ( 22.68)	Acc@1  99.12 ( 99.09)	Acc@3 100.00 (100.00)
[2023-08-24 22:33:42,626 INFO] Epoch: [25][160/169]	Loss 0.07951 (0.07936)	InvT  22.71 ( 22.68)	Acc@1  99.41 ( 99.09)	Acc@3 100.00 (100.00)
[2023-08-24 22:33:51,740 INFO] Learning rate: 2.528159425671494e-05
[2023-08-24 22:33:59,567 INFO] Epoch 25, valid metric: {"Acc@1": 76.038, "Acc@3": 87.541, "loss": 1.158}
[2023-08-24 22:34:04,050 INFO] Delete old checkpoint ./checkpoint/wn18rr/checkpoint_epoch22.mdl
[2023-08-24 22:34:08,238 INFO] Epoch: [26][  0/169]	Loss 0.05788 (0.05788)	InvT  22.72 ( 22.72)	Acc@1  99.51 ( 99.51)	Acc@3 100.00 (100.00)
[2023-08-24 22:34:34,721 INFO] Epoch: [26][ 20/169]	Loss 0.06969 (0.07322)	InvT  22.72 ( 22.72)	Acc@1  99.41 ( 99.14)	Acc@3 100.00 ( 99.99)
[2023-08-24 22:34:59,615 INFO] Epoch: [26][ 40/169]	Loss 0.05841 (0.0718)	InvT  22.73 ( 22.72)	Acc@1  99.61 ( 99.19)	Acc@3 100.00 ( 99.99)
[2023-08-24 22:35:24,586 INFO] Epoch: [26][ 60/169]	Loss 0.06631 (0.07177)	InvT  22.74 ( 22.73)	Acc@1  99.61 ( 99.19)	Acc@3 100.00 ( 99.99)
[2023-08-24 22:35:49,310 INFO] Epoch: [26][ 80/169]	Loss 0.06897 (0.07232)	InvT  22.75 ( 22.73)	Acc@1  99.51 ( 99.18)	Acc@3 100.00 ( 99.99)
[2023-08-24 22:36:14,543 INFO] Epoch: [26][100/169]	Loss 0.07285 (0.07282)	InvT  22.75 ( 22.74)	Acc@1  99.12 ( 99.17)	Acc@3 100.00 ( 99.99)
[2023-08-24 22:36:39,663 INFO] Epoch: [26][120/169]	Loss 0.06833 (0.07291)	InvT  22.76 ( 22.74)	Acc@1  99.51 ( 99.17)	Acc@3 100.00 ( 99.99)
[2023-08-24 22:37:04,685 INFO] Epoch: [26][140/169]	Loss 0.07149 (0.0736)	InvT  22.77 ( 22.74)	Acc@1  99.22 ( 99.16)	Acc@3 100.00 ( 99.99)
[2023-08-24 22:37:29,732 INFO] Epoch: [26][160/169]	Loss 0.08311 (0.07433)	InvT  22.78 ( 22.75)	Acc@1  99.22 ( 99.15)	Acc@3 100.00 ( 99.99)
[2023-08-24 22:37:38,973 INFO] Learning rate: 2.4235672731773734e-05
[2023-08-24 22:37:46,801 INFO] Epoch 26, valid metric: {"Acc@1": 76.335, "Acc@3": 87.409, "loss": 1.156}
[2023-08-24 22:37:51,236 INFO] Delete old checkpoint ./checkpoint/wn18rr/checkpoint_epoch23.mdl
[2023-08-24 22:37:55,367 INFO] Epoch: [27][  0/169]	Loss 0.07428 (0.07428)	InvT  22.78 ( 22.78)	Acc@1  99.02 ( 99.02)	Acc@3 100.00 (100.00)
[2023-08-24 22:38:21,348 INFO] Epoch: [27][ 20/169]	Loss 0.0763 (0.07043)	InvT  22.79 ( 22.78)	Acc@1  99.22 ( 99.25)	Acc@3 100.00 (100.00)
[2023-08-24 22:38:46,609 INFO] Epoch: [27][ 40/169]	Loss 0.07285 (0.06868)	InvT  22.80 ( 22.79)	Acc@1  99.32 ( 99.24)	Acc@3 100.00 (100.00)
[2023-08-24 22:39:11,332 INFO] Epoch: [27][ 60/169]	Loss 0.07821 (0.06951)	InvT  22.80 ( 22.79)	Acc@1  98.63 ( 99.21)	Acc@3 100.00 (100.00)
[2023-08-24 22:39:36,416 INFO] Epoch: [27][ 80/169]	Loss 0.06611 (0.06984)	InvT  22.81 ( 22.80)	Acc@1  99.41 ( 99.22)	Acc@3 100.00 (100.00)
[2023-08-24 22:40:01,363 INFO] Epoch: [27][100/169]	Loss 0.07419 (0.0696)	InvT  22.82 ( 22.80)	Acc@1  99.32 ( 99.23)	Acc@3 100.00 (100.00)
[2023-08-24 22:40:26,236 INFO] Epoch: [27][120/169]	Loss 0.06889 (0.07002)	InvT  22.82 ( 22.80)	Acc@1  99.02 ( 99.21)	Acc@3 100.00 (100.00)
[2023-08-24 22:40:51,061 INFO] Epoch: [27][140/169]	Loss 0.08143 (0.07063)	InvT  22.83 ( 22.81)	Acc@1  99.02 ( 99.20)	Acc@3 100.00 (100.00)
[2023-08-24 22:41:15,834 INFO] Epoch: [27][160/169]	Loss 0.06998 (0.07103)	InvT  22.84 ( 22.81)	Acc@1  99.22 ( 99.19)	Acc@3 100.00 (100.00)
[2023-08-24 22:41:25,004 INFO] Learning rate: 2.318975120683253e-05
[2023-08-24 22:41:32,774 INFO] Epoch 27, valid metric: {"Acc@1": 75.692, "Acc@3": 87.492, "loss": 1.164}
[2023-08-24 22:41:37,231 INFO] Delete old checkpoint ./checkpoint/wn18rr/checkpoint_epoch24.mdl
[2023-08-24 22:41:41,529 INFO] Epoch: [28][  0/169]	Loss 0.05257 (0.05257)	InvT  22.84 ( 22.84)	Acc@1  99.51 ( 99.51)	Acc@3 100.00 (100.00)
[2023-08-24 22:42:07,199 INFO] Epoch: [28][ 20/169]	Loss 0.07459 (0.07144)	InvT  22.85 ( 22.85)	Acc@1  98.93 ( 99.20)	Acc@3 100.00 (100.00)
[2023-08-24 22:42:32,250 INFO] Epoch: [28][ 40/169]	Loss 0.08025 (0.07043)	InvT  22.86 ( 22.85)	Acc@1  99.32 ( 99.23)	Acc@3 100.00 ( 99.99)
[2023-08-24 22:42:57,091 INFO] Epoch: [28][ 60/169]	Loss 0.06588 (0.06843)	InvT  22.86 ( 22.85)	Acc@1  99.22 ( 99.26)	Acc@3 100.00 ( 99.99)
[2023-08-24 22:43:22,512 INFO] Epoch: [28][ 80/169]	Loss 0.07485 (0.06784)	InvT  22.87 ( 22.86)	Acc@1  99.12 ( 99.26)	Acc@3 100.00 ( 99.99)
[2023-08-24 22:43:47,496 INFO] Epoch: [28][100/169]	Loss 0.07573 (0.06713)	InvT  22.88 ( 22.86)	Acc@1  98.73 ( 99.26)	Acc@3 100.00 ( 99.99)
[2023-08-24 22:44:12,435 INFO] Epoch: [28][120/169]	Loss 0.05956 (0.06714)	InvT  22.88 ( 22.86)	Acc@1  99.61 ( 99.27)	Acc@3 100.00 ( 99.99)
[2023-08-24 22:44:37,116 INFO] Epoch: [28][140/169]	Loss 0.06692 (0.06705)	InvT  22.89 ( 22.87)	Acc@1  98.93 ( 99.27)	Acc@3 100.00 ( 99.99)
[2023-08-24 22:45:02,290 INFO] Epoch: [28][160/169]	Loss 0.06823 (0.06719)	InvT  22.90 ( 22.87)	Acc@1  99.32 ( 99.26)	Acc@3 100.00 ( 99.99)
[2023-08-24 22:45:11,452 INFO] Learning rate: 2.2143829681891324e-05
[2023-08-24 22:45:19,241 INFO] Epoch 28, valid metric: {"Acc@1": 75.659, "Acc@3": 87.64, "loss": 1.168}
[2023-08-24 22:45:23,689 INFO] Delete old checkpoint ./checkpoint/wn18rr/checkpoint_epoch25.mdl
[2023-08-24 22:45:27,835 INFO] Epoch: [29][  0/169]	Loss 0.05499 (0.05499)	InvT  22.90 ( 22.90)	Acc@1  99.51 ( 99.51)	Acc@3 100.00 (100.00)
[2023-08-24 22:45:54,215 INFO] Epoch: [29][ 20/169]	Loss 0.04541 (0.06078)	InvT  22.91 ( 22.91)	Acc@1  99.80 ( 99.43)	Acc@3 100.00 (100.00)
[2023-08-24 22:46:19,590 INFO] Epoch: [29][ 40/169]	Loss 0.05438 (0.06105)	InvT  22.92 ( 22.91)	Acc@1  99.71 ( 99.39)	Acc@3 100.00 (100.00)
[2023-08-24 22:46:44,167 INFO] Epoch: [29][ 60/169]	Loss 0.06037 (0.06105)	InvT  22.92 ( 22.91)	Acc@1  99.41 ( 99.36)	Acc@3  99.90 (100.00)
[2023-08-24 22:47:09,314 INFO] Epoch: [29][ 80/169]	Loss 0.04891 (0.06059)	InvT  22.93 ( 22.92)	Acc@1  99.51 ( 99.36)	Acc@3 100.00 (100.00)
[2023-08-24 22:47:34,368 INFO] Epoch: [29][100/169]	Loss 0.07167 (0.06135)	InvT  22.94 ( 22.92)	Acc@1  99.41 ( 99.35)	Acc@3 100.00 (100.00)
[2023-08-24 22:47:59,872 INFO] Epoch: [29][120/169]	Loss 0.05839 (0.06147)	InvT  22.94 ( 22.92)	Acc@1  99.41 ( 99.35)	Acc@3 100.00 (100.00)
[2023-08-24 22:48:24,540 INFO] Epoch: [29][140/169]	Loss 0.08264 (0.06236)	InvT  22.95 ( 22.93)	Acc@1  99.02 ( 99.34)	Acc@3 100.00 (100.00)
[2023-08-24 22:48:49,532 INFO] Epoch: [29][160/169]	Loss 0.07189 (0.06317)	InvT  22.96 ( 22.93)	Acc@1  99.51 ( 99.33)	Acc@3 100.00 (100.00)
[2023-08-24 22:48:58,623 INFO] Learning rate: 2.1097908156950118e-05
[2023-08-24 22:49:06,435 INFO] Epoch 29, valid metric: {"Acc@1": 75.873, "Acc@3": 87.343, "loss": 1.188}
[2023-08-24 22:49:10,897 INFO] Delete old checkpoint ./checkpoint/wn18rr/checkpoint_epoch26.mdl
[2023-08-24 22:49:15,104 INFO] Epoch: [30][  0/169]	Loss 0.06258 (0.06258)	InvT  22.96 ( 22.96)	Acc@1  99.12 ( 99.12)	Acc@3 100.00 (100.00)
[2023-08-24 22:49:40,992 INFO] Epoch: [30][ 20/169]	Loss 0.05214 (0.05624)	InvT  22.97 ( 22.96)	Acc@1  99.61 ( 99.43)	Acc@3 100.00 (100.00)
[2023-08-24 22:50:05,844 INFO] Epoch: [30][ 40/169]	Loss 0.06216 (0.05725)	InvT  22.97 ( 22.97)	Acc@1  99.41 ( 99.41)	Acc@3 100.00 (100.00)
[2023-08-24 22:50:31,146 INFO] Epoch: [30][ 60/169]	Loss 0.0587 (0.05938)	InvT  22.98 ( 22.97)	Acc@1  99.12 ( 99.39)	Acc@3 100.00 (100.00)
[2023-08-24 22:50:56,032 INFO] Epoch: [30][ 80/169]	Loss 0.0683 (0.05963)	InvT  22.98 ( 22.97)	Acc@1  99.32 ( 99.38)	Acc@3 100.00 (100.00)
[2023-08-24 22:51:20,848 INFO] Epoch: [30][100/169]	Loss 0.05526 (0.06026)	InvT  22.99 ( 22.97)	Acc@1  99.32 ( 99.36)	Acc@3 100.00 (100.00)
[2023-08-24 22:51:45,829 INFO] Epoch: [30][120/169]	Loss 0.06349 (0.06045)	InvT  23.00 ( 22.98)	Acc@1  99.22 ( 99.35)	Acc@3 100.00 (100.00)
[2023-08-24 22:52:11,077 INFO] Epoch: [30][140/169]	Loss 0.07359 (0.06067)	InvT  23.00 ( 22.98)	Acc@1  99.02 ( 99.34)	Acc@3 100.00 (100.00)
[2023-08-24 22:52:36,010 INFO] Epoch: [30][160/169]	Loss 0.05597 (0.05998)	InvT  23.01 ( 22.98)	Acc@1  99.32 ( 99.36)	Acc@3 100.00 (100.00)
[2023-08-24 22:52:45,091 INFO] Learning rate: 2.0051986632008913e-05
[2023-08-24 22:52:52,881 INFO] Epoch 30, valid metric: {"Acc@1": 76.17, "Acc@3": 87.063, "loss": 1.183}
[2023-08-24 22:52:57,332 INFO] Delete old checkpoint ./checkpoint/wn18rr/checkpoint_epoch27.mdl
[2023-08-24 22:53:02,948 INFO] Epoch: [31][  0/169]	Loss 0.06338 (0.06338)	InvT  23.01 ( 23.01)	Acc@1  98.93 ( 98.93)	Acc@3 100.00 (100.00)
[2023-08-24 22:53:28,596 INFO] Epoch: [31][ 20/169]	Loss 0.05924 (0.05671)	InvT  23.02 ( 23.02)	Acc@1  99.41 ( 99.40)	Acc@3 100.00 (100.00)
[2023-08-24 22:53:53,207 INFO] Epoch: [31][ 40/169]	Loss 0.04963 (0.05712)	InvT  23.03 ( 23.02)	Acc@1  99.51 ( 99.34)	Acc@3 100.00 (100.00)
[2023-08-24 22:54:18,204 INFO] Epoch: [31][ 60/169]	Loss 0.05753 (0.05571)	InvT  23.03 ( 23.02)	Acc@1  99.22 ( 99.38)	Acc@3 100.00 (100.00)
[2023-08-24 22:54:43,483 INFO] Epoch: [31][ 80/169]	Loss 0.0587 (0.05637)	InvT  23.04 ( 23.03)	Acc@1  99.12 ( 99.37)	Acc@3 100.00 (100.00)
[2023-08-24 22:55:08,211 INFO] Epoch: [31][100/169]	Loss 0.05894 (0.05634)	InvT  23.04 ( 23.03)	Acc@1  99.51 ( 99.37)	Acc@3 100.00 (100.00)
[2023-08-24 22:55:33,124 INFO] Epoch: [31][120/169]	Loss 0.06439 (0.05676)	InvT  23.05 ( 23.03)	Acc@1  99.22 ( 99.37)	Acc@3 100.00 (100.00)
[2023-08-24 22:55:58,313 INFO] Epoch: [31][140/169]	Loss 0.06919 (0.0566)	InvT  23.06 ( 23.03)	Acc@1  99.51 ( 99.38)	Acc@3 100.00 (100.00)
[2023-08-24 22:56:23,228 INFO] Epoch: [31][160/169]	Loss 0.05028 (0.05695)	InvT  23.06 ( 23.04)	Acc@1  99.61 ( 99.37)	Acc@3 100.00 (100.00)
[2023-08-24 22:56:32,489 INFO] Learning rate: 1.9006065107067707e-05
[2023-08-24 22:56:40,295 INFO] Epoch 31, valid metric: {"Acc@1": 75.857, "Acc@3": 86.915, "loss": 1.212}
[2023-08-24 22:56:44,768 INFO] Delete old checkpoint ./checkpoint/wn18rr/checkpoint_epoch28.mdl
[2023-08-24 22:56:48,935 INFO] Epoch: [32][  0/169]	Loss 0.0436 (0.0436)	InvT  23.07 ( 23.07)	Acc@1  99.61 ( 99.61)	Acc@3 100.00 (100.00)
[2023-08-24 22:57:15,391 INFO] Epoch: [32][ 20/169]	Loss 0.06003 (0.0527)	InvT  23.07 ( 23.07)	Acc@1  99.32 ( 99.50)	Acc@3 100.00 (100.00)
[2023-08-24 22:57:40,176 INFO] Epoch: [32][ 40/169]	Loss 0.0463 (0.05298)	InvT  23.08 ( 23.07)	Acc@1  99.61 ( 99.45)	Acc@3 100.00 (100.00)
[2023-08-24 22:58:04,962 INFO] Epoch: [32][ 60/169]	Loss 0.04893 (0.05206)	InvT  23.08 ( 23.07)	Acc@1  99.71 ( 99.48)	Acc@3 100.00 (100.00)
[2023-08-24 22:58:30,167 INFO] Epoch: [32][ 80/169]	Loss 0.05254 (0.0527)	InvT  23.09 ( 23.08)	Acc@1  99.41 ( 99.45)	Acc@3 100.00 (100.00)
[2023-08-24 22:58:55,268 INFO] Epoch: [32][100/169]	Loss 0.04847 (0.05311)	InvT  23.09 ( 23.08)	Acc@1  99.61 ( 99.43)	Acc@3 100.00 (100.00)
[2023-08-24 22:59:20,339 INFO] Epoch: [32][120/169]	Loss 0.04222 (0.05306)	InvT  23.10 ( 23.08)	Acc@1  99.41 ( 99.44)	Acc@3 100.00 (100.00)
[2023-08-24 22:59:45,066 INFO] Epoch: [32][140/169]	Loss 0.05041 (0.05329)	InvT  23.11 ( 23.09)	Acc@1  99.61 ( 99.43)	Acc@3 100.00 (100.00)
[2023-08-24 23:00:10,226 INFO] Epoch: [32][160/169]	Loss 0.05018 (0.05305)	InvT  23.11 ( 23.09)	Acc@1  99.51 ( 99.44)	Acc@3 100.00 (100.00)
[2023-08-24 23:00:19,289 INFO] Learning rate: 1.7960143582126502e-05
[2023-08-24 23:00:27,084 INFO] Epoch 32, valid metric: {"Acc@1": 75.939, "Acc@3": 87.426, "loss": 1.191}
[2023-08-24 23:00:31,557 INFO] Delete old checkpoint ./checkpoint/wn18rr/checkpoint_epoch29.mdl
[2023-08-24 23:00:35,767 INFO] Epoch: [33][  0/169]	Loss 0.04622 (0.04622)	InvT  23.11 ( 23.11)	Acc@1  99.61 ( 99.61)	Acc@3 100.00 (100.00)
[2023-08-24 23:01:01,756 INFO] Epoch: [33][ 20/169]	Loss 0.0441 (0.04899)	InvT  23.12 ( 23.12)	Acc@1  99.51 ( 99.46)	Acc@3 100.00 (100.00)
[2023-08-24 23:01:27,143 INFO] Epoch: [33][ 40/169]	Loss 0.06049 (0.05146)	InvT  23.13 ( 23.12)	Acc@1  99.32 ( 99.43)	Acc@3 100.00 (100.00)
[2023-08-24 23:01:51,879 INFO] Epoch: [33][ 60/169]	Loss 0.05046 (0.05108)	InvT  23.13 ( 23.12)	Acc@1  99.41 ( 99.45)	Acc@3 100.00 (100.00)
[2023-08-24 23:02:16,893 INFO] Epoch: [33][ 80/169]	Loss 0.05723 (0.05145)	InvT  23.14 ( 23.13)	Acc@1  99.32 ( 99.45)	Acc@3 100.00 (100.00)
[2023-08-24 23:02:41,568 INFO] Epoch: [33][100/169]	Loss 0.04847 (0.05211)	InvT  23.14 ( 23.13)	Acc@1  99.71 ( 99.43)	Acc@3 100.00 (100.00)
[2023-08-24 23:03:06,535 INFO] Epoch: [33][120/169]	Loss 0.05413 (0.05178)	InvT  23.15 ( 23.13)	Acc@1  99.32 ( 99.44)	Acc@3 100.00 (100.00)
[2023-08-24 23:03:31,805 INFO] Epoch: [33][140/169]	Loss 0.04828 (0.05183)	InvT  23.15 ( 23.13)	Acc@1  99.22 ( 99.45)	Acc@3 100.00 (100.00)
[2023-08-24 23:03:56,746 INFO] Epoch: [33][160/169]	Loss 0.05464 (0.05189)	InvT  23.16 ( 23.14)	Acc@1  99.32 ( 99.45)	Acc@3 100.00 (100.00)
[2023-08-24 23:04:05,815 INFO] Learning rate: 1.6914222057185297e-05
[2023-08-24 23:04:13,773 INFO] Epoch 33, valid metric: {"Acc@1": 76.005, "Acc@3": 87.376, "loss": 1.192}
[2023-08-24 23:04:18,221 INFO] Delete old checkpoint ./checkpoint/wn18rr/checkpoint_epoch30.mdl
[2023-08-24 23:04:22,461 INFO] Epoch: [34][  0/169]	Loss 0.04526 (0.04526)	InvT  23.16 ( 23.16)	Acc@1  99.61 ( 99.61)	Acc@3 100.00 (100.00)
[2023-08-24 23:04:48,208 INFO] Epoch: [34][ 20/169]	Loss 0.04246 (0.04993)	InvT  23.17 ( 23.17)	Acc@1  99.71 ( 99.51)	Acc@3 100.00 (100.00)
[2023-08-24 23:05:13,151 INFO] Epoch: [34][ 40/169]	Loss 0.04241 (0.04853)	InvT  23.17 ( 23.17)	Acc@1  99.61 ( 99.53)	Acc@3 100.00 (100.00)
[2023-08-24 23:05:38,164 INFO] Epoch: [34][ 60/169]	Loss 0.05393 (0.04938)	InvT  23.18 ( 23.17)	Acc@1  99.90 ( 99.50)	Acc@3 100.00 (100.00)
[2023-08-24 23:06:03,487 INFO] Epoch: [34][ 80/169]	Loss 0.03679 (0.04917)	InvT  23.18 ( 23.17)	Acc@1  99.80 ( 99.49)	Acc@3 100.00 (100.00)
[2023-08-24 23:06:28,287 INFO] Epoch: [34][100/169]	Loss 0.054 (0.04876)	InvT  23.19 ( 23.18)	Acc@1  99.41 ( 99.51)	Acc@3 100.00 (100.00)
[2023-08-24 23:06:53,163 INFO] Epoch: [34][120/169]	Loss 0.05487 (0.04899)	InvT  23.19 ( 23.18)	Acc@1  99.71 ( 99.50)	Acc@3 100.00 (100.00)
[2023-08-24 23:07:18,153 INFO] Epoch: [34][140/169]	Loss 0.05065 (0.04898)	InvT  23.20 ( 23.18)	Acc@1  99.32 ( 99.51)	Acc@3 100.00 (100.00)
[2023-08-24 23:07:42,918 INFO] Epoch: [34][160/169]	Loss 0.04665 (0.04924)	InvT  23.21 ( 23.18)	Acc@1  99.61 ( 99.50)	Acc@3 100.00 (100.00)
[2023-08-24 23:07:52,085 INFO] Learning rate: 1.586830053224409e-05
[2023-08-24 23:07:59,893 INFO] Epoch 34, valid metric: {"Acc@1": 75.692, "Acc@3": 87.113, "loss": 1.215}
[2023-08-24 23:08:04,343 INFO] Delete old checkpoint ./checkpoint/wn18rr/checkpoint_epoch31.mdl
[2023-08-24 23:08:08,650 INFO] Epoch: [35][  0/169]	Loss 0.03876 (0.03876)	InvT  23.21 ( 23.21)	Acc@1  99.80 ( 99.80)	Acc@3 100.00 (100.00)
[2023-08-24 23:08:35,034 INFO] Epoch: [35][ 20/169]	Loss 0.04749 (0.04632)	InvT  23.21 ( 23.21)	Acc@1  99.61 ( 99.53)	Acc@3 100.00 (100.00)
[2023-08-24 23:09:00,033 INFO] Epoch: [35][ 40/169]	Loss 0.03958 (0.04682)	InvT  23.22 ( 23.21)	Acc@1  99.32 ( 99.51)	Acc@3 100.00 (100.00)
[2023-08-24 23:09:25,136 INFO] Epoch: [35][ 60/169]	Loss 0.05059 (0.04594)	InvT  23.22 ( 23.22)	Acc@1  99.22 ( 99.53)	Acc@3 100.00 (100.00)
[2023-08-24 23:09:49,784 INFO] Epoch: [35][ 80/169]	Loss 0.04127 (0.04624)	InvT  23.23 ( 23.22)	Acc@1  99.71 ( 99.51)	Acc@3 100.00 (100.00)
[2023-08-24 23:10:14,853 INFO] Epoch: [35][100/169]	Loss 0.05361 (0.0456)	InvT  23.23 ( 23.22)	Acc@1  99.51 ( 99.53)	Acc@3 100.00 (100.00)
[2023-08-24 23:10:39,719 INFO] Epoch: [35][120/169]	Loss 0.0643 (0.0457)	InvT  23.24 ( 23.22)	Acc@1  99.32 ( 99.53)	Acc@3 100.00 (100.00)
[2023-08-24 23:11:04,389 INFO] Epoch: [35][140/169]	Loss 0.04329 (0.04577)	InvT  23.24 ( 23.23)	Acc@1  99.71 ( 99.53)	Acc@3 100.00 (100.00)
[2023-08-24 23:11:29,240 INFO] Epoch: [35][160/169]	Loss 0.04343 (0.04605)	InvT  23.25 ( 23.23)	Acc@1  99.61 ( 99.54)	Acc@3 100.00 (100.00)
[2023-08-24 23:11:38,434 INFO] Learning rate: 1.4822379007302884e-05
[2023-08-24 23:11:46,207 INFO] Epoch 35, valid metric: {"Acc@1": 76.071, "Acc@3": 87.343, "loss": 1.197}
[2023-08-24 23:11:50,670 INFO] Delete old checkpoint ./checkpoint/wn18rr/checkpoint_epoch32.mdl
[2023-08-24 23:11:54,814 INFO] Epoch: [36][  0/169]	Loss 0.04452 (0.04452)	InvT  23.25 ( 23.25)	Acc@1  99.51 ( 99.51)	Acc@3 100.00 (100.00)
[2023-08-24 23:12:21,009 INFO] Epoch: [36][ 20/169]	Loss 0.05153 (0.04508)	InvT  23.26 ( 23.25)	Acc@1  99.41 ( 99.53)	Acc@3 100.00 (100.00)
[2023-08-24 23:12:46,330 INFO] Epoch: [36][ 40/169]	Loss 0.03656 (0.04512)	InvT  23.26 ( 23.26)	Acc@1  99.71 ( 99.54)	Acc@3 100.00 (100.00)
[2023-08-24 23:13:10,843 INFO] Epoch: [36][ 60/169]	Loss 0.03623 (0.04565)	InvT  23.26 ( 23.26)	Acc@1  99.71 ( 99.49)	Acc@3 100.00 (100.00)
[2023-08-24 23:13:35,867 INFO] Epoch: [36][ 80/169]	Loss 0.05147 (0.04521)	InvT  23.27 ( 23.26)	Acc@1  99.22 ( 99.50)	Acc@3 100.00 (100.00)
[2023-08-24 23:14:00,855 INFO] Epoch: [36][100/169]	Loss 0.04236 (0.04542)	InvT  23.27 ( 23.26)	Acc@1  99.61 ( 99.50)	Acc@3 100.00 (100.00)
[2023-08-24 23:14:25,880 INFO] Epoch: [36][120/169]	Loss 0.05086 (0.0459)	InvT  23.28 ( 23.26)	Acc@1  99.61 ( 99.51)	Acc@3 100.00 (100.00)
[2023-08-24 23:14:50,712 INFO] Epoch: [36][140/169]	Loss 0.04097 (0.04566)	InvT  23.28 ( 23.27)	Acc@1  99.61 ( 99.52)	Acc@3 100.00 (100.00)
[2023-08-24 23:15:15,607 INFO] Epoch: [36][160/169]	Loss 0.04848 (0.04568)	InvT  23.29 ( 23.27)	Acc@1  99.51 ( 99.52)	Acc@3 100.00 (100.00)
[2023-08-24 23:15:24,728 INFO] Learning rate: 1.3776457482361679e-05
[2023-08-24 23:15:32,554 INFO] Epoch 36, valid metric: {"Acc@1": 75.873, "Acc@3": 87.195, "loss": 1.189}
[2023-08-24 23:15:36,961 INFO] Delete old checkpoint ./checkpoint/wn18rr/checkpoint_epoch33.mdl
[2023-08-24 23:15:41,144 INFO] Epoch: [37][  0/169]	Loss 0.03592 (0.03592)	InvT  23.29 ( 23.29)	Acc@1  99.80 ( 99.80)	Acc@3 100.00 (100.00)
[2023-08-24 23:16:07,335 INFO] Epoch: [37][ 20/169]	Loss 0.04685 (0.03924)	InvT  23.30 ( 23.29)	Acc@1  99.61 ( 99.67)	Acc@3 100.00 (100.00)
[2023-08-24 23:16:32,564 INFO] Epoch: [37][ 40/169]	Loss 0.03178 (0.03987)	InvT  23.30 ( 23.30)	Acc@1  99.71 ( 99.65)	Acc@3 100.00 (100.00)
[2023-08-24 23:16:57,623 INFO] Epoch: [37][ 60/169]	Loss 0.04566 (0.03996)	InvT  23.30 ( 23.30)	Acc@1  99.71 ( 99.65)	Acc@3 100.00 (100.00)
[2023-08-24 23:17:22,708 INFO] Epoch: [37][ 80/169]	Loss 0.04099 (0.04124)	InvT  23.31 ( 23.30)	Acc@1  99.80 ( 99.61)	Acc@3 100.00 (100.00)
[2023-08-24 23:17:48,008 INFO] Epoch: [37][100/169]	Loss 0.05081 (0.04127)	InvT  23.31 ( 23.30)	Acc@1  99.32 ( 99.61)	Acc@3 100.00 (100.00)
[2023-08-24 23:18:13,259 INFO] Epoch: [37][120/169]	Loss 0.04785 (0.04182)	InvT  23.32 ( 23.30)	Acc@1  99.51 ( 99.59)	Acc@3 100.00 (100.00)
[2023-08-24 23:18:37,775 INFO] Epoch: [37][140/169]	Loss 0.0519 (0.04228)	InvT  23.32 ( 23.31)	Acc@1  99.22 ( 99.59)	Acc@3 100.00 (100.00)
[2023-08-24 23:19:02,743 INFO] Epoch: [37][160/169]	Loss 0.03847 (0.04245)	InvT  23.33 ( 23.31)	Acc@1  99.80 ( 99.58)	Acc@3 100.00 (100.00)
[2023-08-24 23:19:11,900 INFO] Learning rate: 1.2730535957420472e-05
[2023-08-24 23:19:19,754 INFO] Epoch 37, valid metric: {"Acc@1": 75.593, "Acc@3": 87.426, "loss": 1.22}
[2023-08-24 23:19:24,179 INFO] Delete old checkpoint ./checkpoint/wn18rr/checkpoint_epoch34.mdl
[2023-08-24 23:19:28,367 INFO] Epoch: [38][  0/169]	Loss 0.04198 (0.04198)	InvT  23.33 ( 23.33)	Acc@1  99.51 ( 99.51)	Acc@3 100.00 (100.00)
[2023-08-24 23:19:54,267 INFO] Epoch: [38][ 20/169]	Loss 0.0411 (0.04156)	InvT  23.33 ( 23.33)	Acc@1  99.80 ( 99.57)	Acc@3 100.00 (100.00)
[2023-08-24 23:20:19,142 INFO] Epoch: [38][ 40/169]	Loss 0.03779 (0.04212)	InvT  23.34 ( 23.33)	Acc@1  99.41 ( 99.52)	Acc@3 100.00 (100.00)
[2023-08-24 23:20:44,021 INFO] Epoch: [38][ 60/169]	Loss 0.04658 (0.04156)	InvT  23.34 ( 23.33)	Acc@1  99.41 ( 99.55)	Acc@3 100.00 (100.00)
[2023-08-24 23:21:09,232 INFO] Epoch: [38][ 80/169]	Loss 0.03784 (0.04101)	InvT  23.34 ( 23.34)	Acc@1  99.61 ( 99.56)	Acc@3 100.00 (100.00)
[2023-08-24 23:21:34,216 INFO] Epoch: [38][100/169]	Loss 0.0345 (0.04076)	InvT  23.35 ( 23.34)	Acc@1  99.61 ( 99.56)	Acc@3 100.00 (100.00)
[2023-08-24 23:21:59,048 INFO] Epoch: [38][120/169]	Loss 0.04688 (0.04109)	InvT  23.35 ( 23.34)	Acc@1  99.41 ( 99.55)	Acc@3 100.00 (100.00)
[2023-08-24 23:22:23,640 INFO] Epoch: [38][140/169]	Loss 0.04272 (0.0411)	InvT  23.36 ( 23.34)	Acc@1  99.41 ( 99.56)	Acc@3 100.00 (100.00)
[2023-08-24 23:22:48,912 INFO] Epoch: [38][160/169]	Loss 0.04185 (0.04128)	InvT  23.36 ( 23.34)	Acc@1  99.51 ( 99.55)	Acc@3 100.00 (100.00)
[2023-08-24 23:22:58,207 INFO] Learning rate: 1.1684614432479268e-05
[2023-08-24 23:23:06,036 INFO] Epoch 38, valid metric: {"Acc@1": 76.401, "Acc@3": 87.508, "loss": 1.197}
[2023-08-24 23:23:10,465 INFO] Delete old checkpoint ./checkpoint/wn18rr/checkpoint_epoch35.mdl
[2023-08-24 23:23:14,526 INFO] Epoch: [39][  0/169]	Loss 0.03874 (0.03874)	InvT  23.36 ( 23.36)	Acc@1  99.80 ( 99.80)	Acc@3 100.00 (100.00)
[2023-08-24 23:23:40,580 INFO] Epoch: [39][ 20/169]	Loss 0.03787 (0.04002)	InvT  23.37 ( 23.36)	Acc@1  99.71 ( 99.55)	Acc@3 100.00 (100.00)
[2023-08-24 23:24:05,576 INFO] Epoch: [39][ 40/169]	Loss 0.04637 (0.04042)	InvT  23.37 ( 23.37)	Acc@1  99.51 ( 99.57)	Acc@3 100.00 (100.00)
[2023-08-24 23:24:30,679 INFO] Epoch: [39][ 60/169]	Loss 0.02636 (0.04086)	InvT  23.37 ( 23.37)	Acc@1 100.00 ( 99.58)	Acc@3 100.00 (100.00)
[2023-08-24 23:24:55,730 INFO] Epoch: [39][ 80/169]	Loss 0.0456 (0.04059)	InvT  23.38 ( 23.37)	Acc@1  99.51 ( 99.58)	Acc@3 100.00 (100.00)
[2023-08-24 23:25:20,803 INFO] Epoch: [39][100/169]	Loss 0.03176 (0.04028)	InvT  23.38 ( 23.37)	Acc@1  99.90 ( 99.60)	Acc@3 100.00 (100.00)
[2023-08-24 23:25:45,559 INFO] Epoch: [39][120/169]	Loss 0.03309 (0.04051)	InvT  23.39 ( 23.37)	Acc@1  99.71 ( 99.59)	Acc@3 100.00 (100.00)
[2023-08-24 23:26:10,516 INFO] Epoch: [39][140/169]	Loss 0.03561 (0.04065)	InvT  23.39 ( 23.38)	Acc@1  99.71 ( 99.59)	Acc@3 100.00 (100.00)
[2023-08-24 23:26:35,592 INFO] Epoch: [39][160/169]	Loss 0.04511 (0.0406)	InvT  23.39 ( 23.38)	Acc@1  99.32 ( 99.59)	Acc@3 100.00 (100.00)
[2023-08-24 23:26:44,740 INFO] Learning rate: 1.063869290753806e-05
[2023-08-24 23:26:53,104 INFO] Epoch 39, valid metric: {"Acc@1": 75.692, "Acc@3": 87.195, "loss": 1.216}
[2023-08-24 23:26:57,531 INFO] Delete old checkpoint ./checkpoint/wn18rr/checkpoint_epoch36.mdl
[2023-08-24 23:27:01,732 INFO] Epoch: [40][  0/169]	Loss 0.0321 (0.0321)	InvT  23.40 ( 23.40)	Acc@1  99.90 ( 99.90)	Acc@3 100.00 (100.00)
[2023-08-24 23:27:27,388 INFO] Epoch: [40][ 20/169]	Loss 0.04957 (0.03926)	InvT  23.40 ( 23.40)	Acc@1  99.22 ( 99.61)	Acc@3 100.00 (100.00)
[2023-08-24 23:27:52,476 INFO] Epoch: [40][ 40/169]	Loss 0.03893 (0.03765)	InvT  23.40 ( 23.40)	Acc@1  99.61 ( 99.61)	Acc@3 100.00 (100.00)
[2023-08-24 23:28:17,348 INFO] Epoch: [40][ 60/169]	Loss 0.03256 (0.03793)	InvT  23.41 ( 23.40)	Acc@1  99.90 ( 99.61)	Acc@3 100.00 (100.00)
[2023-08-24 23:28:42,663 INFO] Epoch: [40][ 80/169]	Loss 0.04426 (0.03848)	InvT  23.41 ( 23.40)	Acc@1  99.41 ( 99.61)	Acc@3 100.00 (100.00)
[2023-08-24 23:29:07,353 INFO] Epoch: [40][100/169]	Loss 0.04507 (0.0383)	InvT  23.41 ( 23.40)	Acc@1  99.51 ( 99.61)	Acc@3 100.00 (100.00)
[2023-08-24 23:29:32,263 INFO] Epoch: [40][120/169]	Loss 0.05073 (0.03804)	InvT  23.42 ( 23.41)	Acc@1  99.32 ( 99.61)	Acc@3 100.00 (100.00)
[2023-08-24 23:29:57,083 INFO] Epoch: [40][140/169]	Loss 0.04869 (0.03831)	InvT  23.42 ( 23.41)	Acc@1  99.22 ( 99.60)	Acc@3 100.00 (100.00)
[2023-08-24 23:30:22,201 INFO] Epoch: [40][160/169]	Loss 0.04121 (0.03839)	InvT  23.42 ( 23.41)	Acc@1  99.71 ( 99.60)	Acc@3 100.00 (100.00)
[2023-08-24 23:30:31,403 INFO] Learning rate: 9.592771382596855e-06
[2023-08-24 23:30:39,225 INFO] Epoch 40, valid metric: {"Acc@1": 75.643, "Acc@3": 86.964, "loss": 1.207}
[2023-08-24 23:30:43,636 INFO] Delete old checkpoint ./checkpoint/wn18rr/checkpoint_epoch37.mdl
[2023-08-24 23:30:48,617 INFO] Epoch: [41][  0/169]	Loss 0.047 (0.047)	InvT  23.42 ( 23.42)	Acc@1  99.41 ( 99.41)	Acc@3 100.00 (100.00)
[2023-08-24 23:31:14,339 INFO] Epoch: [41][ 20/169]	Loss 0.02779 (0.03837)	InvT  23.43 ( 23.43)	Acc@1  99.61 ( 99.63)	Acc@3 100.00 (100.00)
[2023-08-24 23:31:39,321 INFO] Epoch: [41][ 40/169]	Loss 0.03171 (0.03888)	InvT  23.43 ( 23.43)	Acc@1  99.80 ( 99.60)	Acc@3 100.00 (100.00)
[2023-08-24 23:32:04,269 INFO] Epoch: [41][ 60/169]	Loss 0.02936 (0.03799)	InvT  23.43 ( 23.43)	Acc@1  99.61 ( 99.61)	Acc@3 100.00 (100.00)
[2023-08-24 23:32:29,199 INFO] Epoch: [41][ 80/169]	Loss 0.04684 (0.03817)	InvT  23.44 ( 23.43)	Acc@1  99.22 ( 99.61)	Acc@3 100.00 (100.00)
[2023-08-24 23:32:54,256 INFO] Epoch: [41][100/169]	Loss 0.03951 (0.03834)	InvT  23.44 ( 23.43)	Acc@1  99.71 ( 99.62)	Acc@3 100.00 (100.00)
[2023-08-24 23:33:19,248 INFO] Epoch: [41][120/169]	Loss 0.04984 (0.03787)	InvT  23.44 ( 23.43)	Acc@1  99.61 ( 99.63)	Acc@3 100.00 (100.00)
[2023-08-24 23:33:44,160 INFO] Epoch: [41][140/169]	Loss 0.04681 (0.03737)	InvT  23.45 ( 23.44)	Acc@1  99.32 ( 99.64)	Acc@3 100.00 (100.00)
[2023-08-24 23:34:09,152 INFO] Epoch: [41][160/169]	Loss 0.02703 (0.03756)	InvT  23.45 ( 23.44)	Acc@1  99.90 ( 99.63)	Acc@3 100.00 (100.00)
[2023-08-24 23:34:18,303 INFO] Learning rate: 8.54684985765565e-06
[2023-08-24 23:34:26,087 INFO] Epoch 41, valid metric: {"Acc@1": 75.956, "Acc@3": 87.146, "loss": 1.214}
[2023-08-24 23:34:30,552 INFO] Delete old checkpoint ./checkpoint/wn18rr/checkpoint_epoch38.mdl
[2023-08-24 23:34:34,700 INFO] Epoch: [42][  0/169]	Loss 0.04215 (0.04215)	InvT  23.45 ( 23.45)	Acc@1  99.51 ( 99.51)	Acc@3 100.00 (100.00)
[2023-08-24 23:35:00,723 INFO] Epoch: [42][ 20/169]	Loss 0.03094 (0.03732)	InvT  23.45 ( 23.45)	Acc@1  99.80 ( 99.65)	Acc@3 100.00 (100.00)
[2023-08-24 23:35:26,176 INFO] Epoch: [42][ 40/169]	Loss 0.03871 (0.03698)	InvT  23.46 ( 23.45)	Acc@1  99.61 ( 99.63)	Acc@3 100.00 (100.00)
[2023-08-24 23:35:50,774 INFO] Epoch: [42][ 60/169]	Loss 0.03589 (0.03645)	InvT  23.46 ( 23.46)	Acc@1  99.61 ( 99.65)	Acc@3 100.00 (100.00)
[2023-08-24 23:36:15,674 INFO] Epoch: [42][ 80/169]	Loss 0.04573 (0.03608)	InvT  23.46 ( 23.46)	Acc@1  99.02 ( 99.64)	Acc@3 100.00 (100.00)
[2023-08-24 23:36:40,840 INFO] Epoch: [42][100/169]	Loss 0.02986 (0.03548)	InvT  23.47 ( 23.46)	Acc@1  99.80 ( 99.66)	Acc@3 100.00 (100.00)
[2023-08-24 23:37:05,454 INFO] Epoch: [42][120/169]	Loss 0.03052 (0.03575)	InvT  23.47 ( 23.46)	Acc@1  99.80 ( 99.65)	Acc@3 100.00 (100.00)
[2023-08-24 23:37:30,480 INFO] Epoch: [42][140/169]	Loss 0.02539 (0.03555)	InvT  23.47 ( 23.46)	Acc@1  99.90 ( 99.66)	Acc@3 100.00 (100.00)
[2023-08-24 23:37:55,303 INFO] Epoch: [42][160/169]	Loss 0.03916 (0.03579)	InvT  23.47 ( 23.46)	Acc@1  99.32 ( 99.65)	Acc@3 100.00 (100.00)
[2023-08-24 23:38:04,418 INFO] Learning rate: 7.500928332714445e-06
[2023-08-24 23:38:12,777 INFO] Epoch 42, valid metric: {"Acc@1": 75.725, "Acc@3": 87.03, "loss": 1.228}
[2023-08-24 23:38:17,231 INFO] Delete old checkpoint ./checkpoint/wn18rr/checkpoint_epoch39.mdl
[2023-08-24 23:38:21,468 INFO] Epoch: [43][  0/169]	Loss 0.0301 (0.0301)	InvT  23.48 ( 23.48)	Acc@1  99.90 ( 99.90)	Acc@3 100.00 (100.00)
[2023-08-24 23:38:47,159 INFO] Epoch: [43][ 20/169]	Loss 0.04071 (0.03221)	InvT  23.48 ( 23.48)	Acc@1  99.41 ( 99.72)	Acc@3 100.00 (100.00)
[2023-08-24 23:39:12,290 INFO] Epoch: [43][ 40/169]	Loss 0.02969 (0.03327)	InvT  23.48 ( 23.48)	Acc@1  99.80 ( 99.70)	Acc@3 100.00 (100.00)
[2023-08-24 23:39:37,351 INFO] Epoch: [43][ 60/169]	Loss 0.03164 (0.03312)	InvT  23.48 ( 23.48)	Acc@1  99.80 ( 99.70)	Acc@3 100.00 (100.00)
[2023-08-24 23:40:02,400 INFO] Epoch: [43][ 80/169]	Loss 0.02905 (0.03328)	InvT  23.49 ( 23.48)	Acc@1 100.00 ( 99.69)	Acc@3 100.00 (100.00)
[2023-08-24 23:40:27,071 INFO] Epoch: [43][100/169]	Loss 0.02901 (0.03346)	InvT  23.49 ( 23.48)	Acc@1  99.80 ( 99.68)	Acc@3 100.00 (100.00)
[2023-08-24 23:40:52,069 INFO] Epoch: [43][120/169]	Loss 0.03138 (0.03369)	InvT  23.49 ( 23.48)	Acc@1  99.71 ( 99.68)	Acc@3 100.00 (100.00)
[2023-08-24 23:41:17,256 INFO] Epoch: [43][140/169]	Loss 0.03224 (0.0337)	InvT  23.49 ( 23.48)	Acc@1  99.80 ( 99.69)	Acc@3 100.00 (100.00)
[2023-08-24 23:41:41,997 INFO] Epoch: [43][160/169]	Loss 0.03849 (0.03369)	InvT  23.50 ( 23.49)	Acc@1  99.41 ( 99.68)	Acc@3 100.00 (100.00)
[2023-08-24 23:41:51,275 INFO] Learning rate: 6.455006807773239e-06
[2023-08-24 23:41:59,059 INFO] Epoch 43, valid metric: {"Acc@1": 75.808, "Acc@3": 87.261, "loss": 1.204}
[2023-08-24 23:42:03,550 INFO] Delete old checkpoint ./checkpoint/wn18rr/checkpoint_epoch40.mdl
[2023-08-24 23:42:07,805 INFO] Epoch: [44][  0/169]	Loss 0.03074 (0.03074)	InvT  23.50 ( 23.50)	Acc@1  99.90 ( 99.90)	Acc@3 100.00 (100.00)
[2023-08-24 23:42:33,695 INFO] Epoch: [44][ 20/169]	Loss 0.03358 (0.03131)	InvT  23.50 ( 23.50)	Acc@1  99.71 ( 99.68)	Acc@3 100.00 (100.00)
[2023-08-24 23:42:59,024 INFO] Epoch: [44][ 40/169]	Loss 0.03473 (0.03258)	InvT  23.50 ( 23.50)	Acc@1  99.71 ( 99.67)	Acc@3 100.00 (100.00)
[2023-08-24 23:43:23,678 INFO] Epoch: [44][ 60/169]	Loss 0.02468 (0.03207)	InvT  23.50 ( 23.50)	Acc@1 100.00 ( 99.69)	Acc@3 100.00 (100.00)
[2023-08-24 23:43:48,701 INFO] Epoch: [44][ 80/169]	Loss 0.03552 (0.03291)	InvT  23.51 ( 23.50)	Acc@1  99.71 ( 99.69)	Acc@3 100.00 (100.00)
[2023-08-24 23:44:13,557 INFO] Epoch: [44][100/169]	Loss 0.0369 (0.0328)	InvT  23.51 ( 23.50)	Acc@1  99.71 ( 99.70)	Acc@3 100.00 (100.00)
[2023-08-24 23:44:38,519 INFO] Epoch: [44][120/169]	Loss 0.03165 (0.03273)	InvT  23.51 ( 23.50)	Acc@1  99.61 ( 99.69)	Acc@3 100.00 (100.00)
[2023-08-24 23:45:03,435 INFO] Epoch: [44][140/169]	Loss 0.02836 (0.03299)	InvT  23.51 ( 23.50)	Acc@1  99.51 ( 99.69)	Acc@3 100.00 (100.00)
[2023-08-24 23:45:28,388 INFO] Epoch: [44][160/169]	Loss 0.02933 (0.03318)	InvT  23.51 ( 23.51)	Acc@1  99.90 ( 99.69)	Acc@3 100.00 (100.00)
[2023-08-24 23:45:37,515 INFO] Learning rate: 5.409085282832034e-06
[2023-08-24 23:45:45,334 INFO] Epoch 44, valid metric: {"Acc@1": 75.577, "Acc@3": 86.75, "loss": 1.232}
[2023-08-24 23:45:49,819 INFO] Delete old checkpoint ./checkpoint/wn18rr/checkpoint_epoch41.mdl
[2023-08-24 23:45:55,342 INFO] Epoch: [45][  0/169]	Loss 0.03252 (0.03252)	InvT  23.51 ( 23.51)	Acc@1  99.61 ( 99.61)	Acc@3 100.00 (100.00)
[2023-08-24 23:46:20,604 INFO] Epoch: [45][ 20/169]	Loss 0.03204 (0.03179)	InvT  23.52 ( 23.52)	Acc@1  99.61 ( 99.71)	Acc@3 100.00 (100.00)
[2023-08-24 23:46:45,556 INFO] Epoch: [45][ 40/169]	Loss 0.02827 (0.03186)	InvT  23.52 ( 23.52)	Acc@1  99.61 ( 99.68)	Acc@3 100.00 (100.00)
[2023-08-24 23:47:10,416 INFO] Epoch: [45][ 60/169]	Loss 0.03475 (0.03239)	InvT  23.52 ( 23.52)	Acc@1  99.80 ( 99.68)	Acc@3 100.00 (100.00)
[2023-08-24 23:47:35,691 INFO] Epoch: [45][ 80/169]	Loss 0.03883 (0.03217)	InvT  23.52 ( 23.52)	Acc@1  99.51 ( 99.69)	Acc@3 100.00 (100.00)
[2023-08-24 23:48:00,294 INFO] Epoch: [45][100/169]	Loss 0.03263 (0.03193)	InvT  23.52 ( 23.52)	Acc@1  99.71 ( 99.70)	Acc@3 100.00 (100.00)
[2023-08-24 23:48:25,316 INFO] Epoch: [45][120/169]	Loss 0.03061 (0.03202)	InvT  23.53 ( 23.52)	Acc@1  99.80 ( 99.71)	Acc@3 100.00 (100.00)
[2023-08-24 23:48:50,115 INFO] Epoch: [45][140/169]	Loss 0.02817 (0.0318)	InvT  23.53 ( 23.52)	Acc@1  99.80 ( 99.71)	Acc@3 100.00 (100.00)
[2023-08-24 23:49:15,187 INFO] Epoch: [45][160/169]	Loss 0.02762 (0.03178)	InvT  23.53 ( 23.52)	Acc@1  99.61 ( 99.70)	Acc@3 100.00 (100.00)
[2023-08-24 23:49:24,363 INFO] Learning rate: 4.3631637578908285e-06
[2023-08-24 23:49:32,194 INFO] Epoch 45, valid metric: {"Acc@1": 75.873, "Acc@3": 86.833, "loss": 1.209}
[2023-08-24 23:49:36,668 INFO] Delete old checkpoint ./checkpoint/wn18rr/checkpoint_epoch42.mdl
[2023-08-24 23:49:40,781 INFO] Epoch: [46][  0/169]	Loss 0.02523 (0.02523)	InvT  23.53 ( 23.53)	Acc@1  99.71 ( 99.71)	Acc@3 100.00 (100.00)
[2023-08-24 23:50:07,315 INFO] Epoch: [46][ 20/169]	Loss 0.02919 (0.03275)	InvT  23.53 ( 23.53)	Acc@1  99.80 ( 99.64)	Acc@3 100.00 (100.00)
[2023-08-24 23:50:32,369 INFO] Epoch: [46][ 40/169]	Loss 0.033 (0.03142)	InvT  23.53 ( 23.53)	Acc@1  99.71 ( 99.68)	Acc@3 100.00 (100.00)
[2023-08-24 23:50:57,384 INFO] Epoch: [46][ 60/169]	Loss 0.03007 (0.03198)	InvT  23.53 ( 23.53)	Acc@1  99.61 ( 99.66)	Acc@3 100.00 (100.00)
[2023-08-24 23:51:22,247 INFO] Epoch: [46][ 80/169]	Loss 0.02175 (0.0316)	InvT  23.54 ( 23.53)	Acc@1  99.90 ( 99.67)	Acc@3 100.00 (100.00)
[2023-08-24 23:51:47,442 INFO] Epoch: [46][100/169]	Loss 0.03527 (0.03156)	InvT  23.54 ( 23.53)	Acc@1  99.61 ( 99.67)	Acc@3 100.00 (100.00)
[2023-08-24 23:52:12,461 INFO] Epoch: [46][120/169]	Loss 0.02714 (0.03154)	InvT  23.54 ( 23.53)	Acc@1  99.71 ( 99.68)	Acc@3 100.00 (100.00)
[2023-08-24 23:52:37,154 INFO] Epoch: [46][140/169]	Loss 0.03028 (0.03154)	InvT  23.54 ( 23.53)	Acc@1 100.00 ( 99.68)	Acc@3 100.00 (100.00)
[2023-08-24 23:53:02,172 INFO] Epoch: [46][160/169]	Loss 0.03603 (0.03167)	InvT  23.54 ( 23.54)	Acc@1  99.41 ( 99.68)	Acc@3 100.00 (100.00)
[2023-08-24 23:53:11,453 INFO] Learning rate: 3.3172422329496223e-06
[2023-08-24 23:53:19,247 INFO] Epoch 46, valid metric: {"Acc@1": 75.643, "Acc@3": 86.997, "loss": 1.243}
[2023-08-24 23:53:23,661 INFO] Delete old checkpoint ./checkpoint/wn18rr/checkpoint_epoch43.mdl
[2023-08-24 23:53:27,819 INFO] Epoch: [47][  0/169]	Loss 0.02571 (0.02571)	InvT  23.54 ( 23.54)	Acc@1  99.71 ( 99.71)	Acc@3 100.00 (100.00)
[2023-08-24 23:53:54,014 INFO] Epoch: [47][ 20/169]	Loss 0.04329 (0.03055)	InvT  23.54 ( 23.54)	Acc@1  99.32 ( 99.70)	Acc@3 100.00 (100.00)
[2023-08-24 23:54:19,272 INFO] Epoch: [47][ 40/169]	Loss 0.02725 (0.03013)	InvT  23.54 ( 23.54)	Acc@1  99.80 ( 99.73)	Acc@3 100.00 (100.00)
[2023-08-24 23:54:44,112 INFO] Epoch: [47][ 60/169]	Loss 0.03518 (0.03073)	InvT  23.54 ( 23.54)	Acc@1  99.41 ( 99.69)	Acc@3 100.00 (100.00)
[2023-08-24 23:55:09,043 INFO] Epoch: [47][ 80/169]	Loss 0.02889 (0.03077)	InvT  23.55 ( 23.54)	Acc@1  99.80 ( 99.69)	Acc@3 100.00 (100.00)
[2023-08-24 23:55:33,964 INFO] Epoch: [47][100/169]	Loss 0.0322 (0.03077)	InvT  23.55 ( 23.54)	Acc@1  99.71 ( 99.70)	Acc@3 100.00 (100.00)
[2023-08-24 23:55:59,034 INFO] Epoch: [47][120/169]	Loss 0.02855 (0.03101)	InvT  23.55 ( 23.54)	Acc@1  99.90 ( 99.70)	Acc@3 100.00 (100.00)
[2023-08-24 23:56:24,120 INFO] Epoch: [47][140/169]	Loss 0.02911 (0.03107)	InvT  23.55 ( 23.55)	Acc@1  99.80 ( 99.70)	Acc@3 100.00 (100.00)
[2023-08-24 23:56:49,133 INFO] Epoch: [47][160/169]	Loss 0.04663 (0.0313)	InvT  23.55 ( 23.55)	Acc@1  99.41 ( 99.70)	Acc@3 100.00 (100.00)
[2023-08-24 23:56:58,390 INFO] Learning rate: 2.271320708008417e-06
[2023-08-24 23:57:06,197 INFO] Epoch 47, valid metric: {"Acc@1": 75.264, "Acc@3": 87.129, "loss": 1.238}
[2023-08-24 23:57:10,659 INFO] Delete old checkpoint ./checkpoint/wn18rr/checkpoint_epoch44.mdl
[2023-08-24 23:57:14,930 INFO] Epoch: [48][  0/169]	Loss 0.0267 (0.0267)	InvT  23.55 ( 23.55)	Acc@1  99.80 ( 99.80)	Acc@3 100.00 (100.00)
[2023-08-24 23:57:41,306 INFO] Epoch: [48][ 20/169]	Loss 0.04024 (0.02915)	InvT  23.55 ( 23.55)	Acc@1  99.51 ( 99.74)	Acc@3 100.00 (100.00)
[2023-08-24 23:58:06,154 INFO] Epoch: [48][ 40/169]	Loss 0.03138 (0.02959)	InvT  23.55 ( 23.55)	Acc@1  99.90 ( 99.75)	Acc@3 100.00 (100.00)
[2023-08-24 23:58:31,372 INFO] Epoch: [48][ 60/169]	Loss 0.03055 (0.02959)	InvT  23.55 ( 23.55)	Acc@1  99.41 ( 99.73)	Acc@3 100.00 (100.00)
[2023-08-24 23:58:56,331 INFO] Epoch: [48][ 80/169]	Loss 0.02456 (0.02946)	InvT  23.55 ( 23.55)	Acc@1  99.80 ( 99.74)	Acc@3 100.00 (100.00)
[2023-08-24 23:59:20,982 INFO] Epoch: [48][100/169]	Loss 0.02622 (0.02973)	InvT  23.55 ( 23.55)	Acc@1  99.80 ( 99.73)	Acc@3 100.00 (100.00)
[2023-08-24 23:59:46,475 INFO] Epoch: [48][120/169]	Loss 0.03 (0.02986)	InvT  23.55 ( 23.55)	Acc@1  99.90 ( 99.73)	Acc@3 100.00 (100.00)
[2023-08-25 00:00:11,411 INFO] Epoch: [48][140/169]	Loss 0.0318 (0.02978)	InvT  23.56 ( 23.55)	Acc@1  99.61 ( 99.73)	Acc@3 100.00 (100.00)
[2023-08-25 00:00:36,132 INFO] Epoch: [48][160/169]	Loss 0.04986 (0.03009)	InvT  23.56 ( 23.55)	Acc@1  99.41 ( 99.72)	Acc@3 100.00 (100.00)
[2023-08-25 00:00:45,340 INFO] Learning rate: 1.2253991830672115e-06
[2023-08-25 00:00:53,129 INFO] Epoch 48, valid metric: {"Acc@1": 75.445, "Acc@3": 86.783, "loss": 1.228}
[2023-08-25 00:00:57,624 INFO] Delete old checkpoint ./checkpoint/wn18rr/checkpoint_epoch45.mdl
[2023-08-25 00:01:01,881 INFO] Epoch: [49][  0/169]	Loss 0.02295 (0.02295)	InvT  23.56 ( 23.56)	Acc@1  99.90 ( 99.90)	Acc@3 100.00 (100.00)
[2023-08-25 00:01:28,219 INFO] Epoch: [49][ 20/169]	Loss 0.02423 (0.02776)	InvT  23.56 ( 23.56)	Acc@1  99.80 ( 99.80)	Acc@3 100.00 (100.00)
[2023-08-25 00:01:53,248 INFO] Epoch: [49][ 40/169]	Loss 0.03864 (0.02822)	InvT  23.56 ( 23.56)	Acc@1  99.51 ( 99.78)	Acc@3 100.00 (100.00)
[2023-08-25 00:02:18,395 INFO] Epoch: [49][ 60/169]	Loss 0.03585 (0.0292)	InvT  23.56 ( 23.56)	Acc@1  99.41 ( 99.76)	Acc@3 100.00 (100.00)
[2023-08-25 00:02:42,955 INFO] Epoch: [49][ 80/169]	Loss 0.02523 (0.02893)	InvT  23.56 ( 23.56)	Acc@1  99.80 ( 99.76)	Acc@3 100.00 (100.00)
[2023-08-25 00:03:08,164 INFO] Epoch: [49][100/169]	Loss 0.02801 (0.02903)	InvT  23.56 ( 23.56)	Acc@1  99.61 ( 99.75)	Acc@3 100.00 (100.00)
[2023-08-25 00:03:33,222 INFO] Epoch: [49][120/169]	Loss 0.02697 (0.02925)	InvT  23.56 ( 23.56)	Acc@1  99.71 ( 99.75)	Acc@3 100.00 (100.00)
[2023-08-25 00:03:57,706 INFO] Epoch: [49][140/169]	Loss 0.02525 (0.02926)	InvT  23.56 ( 23.56)	Acc@1  99.71 ( 99.74)	Acc@3 100.00 (100.00)
[2023-08-25 00:04:22,632 INFO] Epoch: [49][160/169]	Loss 0.02656 (0.02914)	InvT  23.56 ( 23.56)	Acc@1  99.80 ( 99.75)	Acc@3 100.00 (100.00)
[2023-08-25 00:04:32,001 INFO] Learning rate: 1.794776581260057e-07
[2023-08-25 00:04:39,799 INFO] Epoch 49, valid metric: {"Acc@1": 76.137, "Acc@3": 87.063, "loss": 1.213}
[2023-08-25 00:04:44,301 INFO] Delete old checkpoint ./checkpoint/wn18rr/checkpoint_epoch46.mdl
start time :2023-08-24 20:53:37.530794 
end time :2023-08-25 00:04:44.349884 
total time :11466.81909942627
+ --max-to-keep 3
scripts/train_wn.sh: line 38: --max-to-keep: command not found
这个工作完成啦
